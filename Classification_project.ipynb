{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******Classification of Adult Data Set Begins ******\n",
      "READING TRAIN DATA\n",
      "Pre Processing Train data with One Hot Encoding\n",
      "OneHotEncoder(categorical_features=None,\n",
      "              categories=[['Private', 'Self-emp-not-inc', 'Self-emp-inc',\n",
      "                           'Federal-gov', 'Local-gov', 'State-gov',\n",
      "                           'Without-pay', 'Never-worked'],\n",
      "                          ['Bachelors', 'Some-college', '11th', 'HS-grad',\n",
      "                           'Prof-school', 'Assoc-acdm', 'Assoc-voc', '9th',\n",
      "                           '7th-8th', '12th', 'Masters', '1st-4th', '10th',\n",
      "                           'Doctorate', '5th-6th', 'Preschool'],\n",
      "                          ['Married-civ-sp...\n",
      "                           'Puerto-Rico', 'Canada', 'Germany',\n",
      "                           'Outlying-US(Guam-USVI-etc)', 'India', 'Japan',\n",
      "                           'Greece', 'South', 'China', 'Cuba', 'Iran',\n",
      "                           'Honduras', 'Philippines', 'Italy', 'Poland',\n",
      "                           'Jamaica', 'Vietnam', 'Mexico', 'Portugal',\n",
      "                           'Ireland', 'France', 'Dominican-Republic', 'Laos',\n",
      "                           'Ecuador', 'Taiwan', 'Haiti', 'Columbia', ...]],\n",
      "              drop=None, dtype=<class 'numpy.float64'>, handle_unknown='ignore',\n",
      "              n_values=None, sparse=False)\n",
      "Normalizing data with Standard Scaler\n",
      "Reducing dimensionality to improve classifier run time\n",
      "-- Training KNN --\n",
      "Starting KNN classification- Expected to take about 5 mins as its a hude data set\n",
      "Calling random_cv\n",
      "Starting search\n",
      "Score mechanism implemented by RandomizedCV - AUROC score\n",
      "Fitting 3 folds for each of 4 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  12 out of  12 | elapsed:  3.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'weights': 'distance', 'n_neighbors': 29, 'leaf_size': 65, 'algorithm': 'kd_tree'}\n",
      "73.6% accuracy on validation sets (average)\n",
      "Complete Training Accuracy\n",
      "0.9999692884125181\n",
      "--Training SVM \n",
      "Starting SVM classification\n",
      "Starting search\n",
      "Score mechanism implemented by RandomizedCV - AUROC score\n",
      "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  30 out of  30 | elapsed:  1.5min finished\n",
      "C:\\Users\\sadiq\\Miniconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'C': 3.887126993208802, 'dual': True, 'max_iter': 3200}\n",
      "76.5% accuracy on validation sets (average)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sadiq\\Miniconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete Training Accuracy\n",
      "0.8521237062743773\n",
      " Preprocessing with Ordinal Encoding for Decison Tree Algorithms\n",
      "--Training Decision Trees--\n",
      "Starting search\n",
      "Score mechanism implemented by RandomizedCV - AUROC score\n",
      "Fitting 5 folds for each of 15 candidates, totalling 75 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=4)]: Done  75 out of  75 | elapsed:    4.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'splitter': 'best', 'max_features': None, 'max_depth': 500, 'criterion': 'entropy'}\n",
      "74.9% accuracy on validation sets (average)\n",
      "Complete Training Accuracy\n",
      "0.9999692884125181\n",
      "--Training Random Forests--\n",
      "Random Forest Classifier Called\n",
      "Starting search\n",
      "Score mechanism implemented by RandomizedCV - AUROC score\n",
      "Fitting 3 folds for each of 12 candidates, totalling 36 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  36 out of  36 | elapsed:  1.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'warm_start': True, 'n_jobs': 4, 'n_estimators': 175, 'max_features': 'log2', 'max_depth': 500, 'criterion': 'entropy'}\n",
      "77.9% accuracy on validation sets (average)\n",
      "Complete Training Accuracy\n",
      "0.9999692884125181\n",
      "--Training AdaBoost-- \n",
      "AdaBoost Classifier Called\n",
      "Starting search\n",
      "Score mechanism implemented by RandomizedCV - AUROC score\n",
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:  3.2min\n",
      "[Parallel(n_jobs=4)]: Done  50 out of  50 | elapsed:  4.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'random_state': 0, 'n_estimators': 750, 'algorithm': 'SAMME.R'}\n",
      "79.0% accuracy on validation sets (average)\n",
      "Complete Training Accuracy\n",
      "0.8727311814747705\n",
      "--Training Logistic Reg--\n",
      "Logistic Reg Classifier Called\n",
      "Starting search\n",
      "Score mechanism implemented by RandomizedCV - AUROC score\n",
      "Fitting 3 folds for each of 15 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  45 out of  45 | elapsed:   12.0s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'C': 15.308517378672184, 'fit_intercept': False, 'max_iter': 9000, 'penalty': 'l2', 'solver': 'newton-cg'}\n",
      "78.6% accuracy on validation sets (average)\n",
      "Complete Training Accuracy\n",
      "0.7192346672399497\n",
      "--Training Naive Bayes-- \n",
      "Naive Bayes Classifier called\n",
      "Starting search\n",
      "Score mechanism implemented by RandomizedCV - AUROC score\n",
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done  25 out of  25 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'priors': None}\n",
      "74.8% accuracy on validation sets (average)\n",
      "Complete Training Accuracy\n",
      "0.6838856300482172\n",
      "--READING TEST DATA\n",
      "One Hot Encoding of Data\n",
      "OneHotEncoder(categorical_features=None,\n",
      "              categories=[['Private', 'Self-emp-not-inc', 'Self-emp-inc',\n",
      "                           'Federal-gov', 'Local-gov', 'State-gov',\n",
      "                           'Without-pay', 'Never-worked'],\n",
      "                          ['Bachelors', 'Some-college', '11th', 'HS-grad',\n",
      "                           'Prof-school', 'Assoc-acdm', 'Assoc-voc', '9th',\n",
      "                           '7th-8th', '12th', 'Masters', '1st-4th', '10th',\n",
      "                           'Doctorate', '5th-6th', 'Preschool'],\n",
      "                          ['Married-civ-sp...\n",
      "                           'Puerto-Rico', 'Canada', 'Germany',\n",
      "                           'Outlying-US(Guam-USVI-etc)', 'India', 'Japan',\n",
      "                           'Greece', 'South', 'China', 'Cuba', 'Iran',\n",
      "                           'Honduras', 'Philippines', 'Italy', 'Poland',\n",
      "                           'Jamaica', 'Vietnam', 'Mexico', 'Portugal',\n",
      "                           'Ireland', 'France', 'Dominican-Republic', 'Laos',\n",
      "                           'Ecuador', 'Taiwan', 'Haiti', 'Columbia', ...]],\n",
      "              drop=None, dtype=<class 'numpy.float64'>, handle_unknown='ignore',\n",
      "              n_values=None, sparse=False)\n",
      "Reducing dimensionality to improve classifier run time\n",
      "**Test Data Prediction Begins*\n",
      "Testing KNN Classifier\n",
      "0.812419384558688\n",
      "Testing SVM Classifier\n",
      "0.8522818008721823\n",
      "Testing Decision Trees\n",
      "0.8162275044530434\n",
      "Testing Random Forests\n",
      "0.854861494994165\n",
      "Testing Adaboost\n",
      "0.8692954978195443\n",
      "Testing Logistic Regression\n",
      "0.6889011731466126\n",
      "Testing Naive Bayes\n",
      "0.7330016583747927\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from io import StringIO\n",
    "import scipy\n",
    "import scipy.stats               # For reciprocal distribution\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import svm\n",
    "import sklearn.tree        # For DecisionTreeClassifier class\n",
    "import sklearn.ensemble    # For RandomForestClassifier class\n",
    "import sklearn.linear_model # For Logistic Classifier\n",
    "#from sklearn.neighbors import LSHForest\n",
    "import sklearn.naive_bayes #For Naive Bayes\n",
    "import sklearn.neural_network #For MLP classifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # Ignore sklearn deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)       # Ignore sklearn deprecation warnings\n",
    "np.set_printoptions(precision=20, suppress=True)\n",
    "\n",
    "class Adult:\n",
    "    #Removing Race and Sex Variable form data for fairness purpose\n",
    "    def preprocess_data_OH(self,X,X_num,y):\n",
    "        X= X.tolist()\n",
    "        #print(X[0])\n",
    "        workclass = ['Private','Self-emp-not-inc','Self-emp-inc','Federal-gov','Local-gov','State-gov','Without-pay','Never-worked']\n",
    "        education = ['Bachelors','Some-college','11th','HS-grad','Prof-school','Assoc-acdm','Assoc-voc','9th','7th-8th','12th','Masters','1st-4th','10th','Doctorate','5th-6th','Preschool']\n",
    "        marital_status = ['Married-civ-spouse','Divorced','Never-married','Separated','Widowed','Married-spouse-absent','Married-AF-spouse']\n",
    "        occupation = ['Tech-support','Craft-repair','Other-service','Sales','Exec-managerial','Prof-specialty','Handlers-cleaners','Machine-op-inspct','Adm-clerical','Farming-fishing','Transport-moving','Priv-house-serv','Protective-serv','Armed-Forces']\n",
    "        relationship = ['Wife','Own-child','Husband','Not-in-family','Other-relative','Unmarried']\n",
    "        native_country = ['United-States','Cambodia','England','Puerto-Rico','Canada','Germany','Outlying-US(Guam-USVI-etc)','India','Japan','Greece','South','China','Cuba','Iran','Honduras','Philippines','Italy','Poland','Jamaica','Vietnam','Mexico','Portugal','Ireland','France','Dominican-Republic','Laos','Ecuador','Taiwan','Haiti','Columbia','Hungary','Guatemala','Nicaragua','Scotland','Thailand','Yugoslavia','El-Salvador','Trinadad&Tobago','Peru','Hong','Holand-Netherlands']\n",
    "        \n",
    "        encoder = preprocessing.OneHotEncoder(categories=[workclass, education, marital_status,occupation,relationship,native_country],\n",
    "                                              handle_unknown='ignore',sparse=False)\n",
    "        print(encoder.fit(X))\n",
    "        X = encoder.transform(X)\n",
    "        X = np.column_stack((X_num,X)) #Combine numeric and encoded string input data\n",
    "        y[y=='<=50K']= 0\n",
    "        y[y=='>50K'] = 1\n",
    "        y[y=='<=50K.']= 0\n",
    "        y[y=='>50K.'] = 1\n",
    "        y = y.astype(int)\n",
    "        return (X,y)\n",
    "    \n",
    "    def scale_data(self,X):\n",
    "        scaler = preprocessing.StandardScaler(with_mean = False).fit(X)\n",
    "        X = scaler.transform(X)\n",
    "        return(X,scaler)\n",
    "    \n",
    "    def preprocess_data_OE(self,X,X_num,y):\n",
    "        X = X.tolist()\n",
    "        encoder = preprocessing.OrdinalEncoder()\n",
    "        encoder.fit(X)\n",
    "        X = encoder.transform(X)\n",
    "        X = np.column_stack((X_num,X))\n",
    "        y[y=='<=50K.']= 0\n",
    "        y[y=='<=50K.']= 0\n",
    "        y[y=='>50K.'] = 1\n",
    "        y[y=='<=50K.']= 0\n",
    "        y = y.astype(int)\n",
    "        return(X,y)\n",
    "    \n",
    "    def __init__(self):\n",
    "        #write your actual run code\n",
    "        pass\n",
    "    def random_CV(self,clf,X,y,param_grid,n_iter,cv):\n",
    "        scorer_AUROC = make_scorer(sklearn.metrics.roc_auc_score) #using ROC for scoring criteria\n",
    "        print(\"Starting search\")\n",
    "        print(\"Score mechanism implemented by RandomizedCV - AUROC score\")\n",
    "        random_search = model_selection.RandomizedSearchCV(clf, param_distributions = param_grid,n_iter = n_iter, cv = cv,\n",
    "                                           iid = False,verbose=1,scoring = scorer_AUROC,n_jobs = 4)\n",
    "        random_search.fit(X, y)\n",
    "        print(\"best parameters:\", random_search.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (random_search.best_score_*100))\n",
    "        return random_search.best_params_\n",
    "    \n",
    "    def KNN(self,X,y):\n",
    "        print(\"Starting KNN classification- Expected to take about 5 mins as its a hude data set\")\n",
    "       \n",
    "       # X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.2)\n",
    "        knn_clf = KNeighborsClassifier()\n",
    "        param_dist = {'n_neighbors': range(1,100),\n",
    "                     \"algorithm\" : ['ball_tree', 'kd_tree'],\n",
    "                    \"weights\" : ['uniform', 'distance'],\n",
    "                    \"leaf_size\" : range(1,100)}\n",
    "        print(\"Calling random_cv\")\n",
    "        return self.random_CV(knn_clf,X,y,param_dist,4,3)\n",
    "        \n",
    "        \n",
    "    def SVM_clf(self,X,y):\n",
    "        print(\"Starting SVM classification\")\n",
    "        #svm_clf = svm.SVC()\n",
    "        svm_clf = svm.LinearSVC()\n",
    "        param_dist = {\n",
    "            'C'     : scipy.stats.reciprocal(1.0, 1000.),\n",
    "            #'penalty': ['l1','l2'],\n",
    "            'dual': [True,False],\n",
    "            'max_iter' : np.arange(2000,10000,200)\n",
    "            }\n",
    "        return self.random_CV(svm_clf,X,y,param_dist,10,3)\n",
    "        \n",
    "    def DT_clf(self,X,y):\n",
    "        tree_clf = sklearn.tree.DecisionTreeClassifier()\n",
    "        param_dist = {\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"splitter\" : ['best', 'random'],\n",
    "            \"max_depth\" :[None,500,750,1000,1500,2000],\n",
    "            \"max_features\": [\"sqrt\",\"log2\",None]\n",
    "        }\n",
    "        return self.random_CV(tree_clf,X,y,param_dist,15,5)\n",
    "    \n",
    "    def RF_clf(self,X,y):\n",
    "        print(\"Random Forest Classifier Called\")\n",
    "        rf_clf = sklearn.ensemble.RandomForestClassifier()\n",
    "        param_dist = {\n",
    "            \"n_estimators\" : [10,25,50,75,100,125,150,175,200],\n",
    "            \"max_depth\" :[None,500,750,1000,1500,2000],\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"max_features\": [\"sqrt\",\"log2\",None],\n",
    "            \"n_jobs\": [4],\n",
    "            \"warm_start\" : [True]\n",
    "           # \"bootstrap\": [True,False]\n",
    "        }\n",
    "        return self.random_CV(rf_clf,X,y,param_dist,12,3)\n",
    "        \n",
    "        \n",
    "    def ADB_clf(self,X,y):\n",
    "        print(\"AdaBoost Classifier Called\")\n",
    "        adb_clf = sklearn.ensemble.AdaBoostClassifier()\n",
    "        param_dist = {\n",
    "            \"n_estimators\" : [50,100,150,200,300,500,750,1000],\n",
    "            \"algorithm\" : ['SAMME', 'SAMME.R'],\n",
    "            \"random_state\" : [0]\n",
    "        }\n",
    "        return self.random_CV(adb_clf,X,y,param_dist,5,5)\n",
    "        \n",
    "    \n",
    "    def LR_clf(self,X,y):\n",
    "        print(\"Logistic Reg Classifier Called\")\n",
    "        lr_clf = sklearn.linear_model.LogisticRegression()\n",
    "        param_dist = {\n",
    "            \"fit_intercept\" : [True, False],\n",
    "            'C'     : scipy.stats.reciprocal(1.0, 1000.),\n",
    "            \"solver\" : ['lbfgs','sag','saga','newton-cg'],\n",
    "            \"penalty\" : ['l2'],\n",
    "            'max_iter' : np.arange(500,10000,500)\n",
    "        }\n",
    "        return self.random_CV(lr_clf,X,y,param_dist,15,3)\n",
    "        \n",
    "    \n",
    "    def NB_clf(self,X,y):\n",
    "        print(\"Naive Bayes Classifier called\")\n",
    "        nb_clf = sklearn.naive_bayes.GaussianNB()\n",
    "        param_dist = {\n",
    "            \"priors\": [None,[0.5,0.5],[0.6,0.4],[0.4,0.6],[0.3,0.7],[0.7,0.3]]\n",
    "        }\n",
    "        return self.random_CV(nb_clf,X,y,param_dist,5,5)\n",
    "\n",
    "    \n",
    "    def MLP_clf(self,X,y):\n",
    "        print(\"Neural Network / MLP Classifier called\")\n",
    "        mlp_clf = sklearn.neural_network.MLPClassifier()\n",
    "        param_dist = {\n",
    "            \"hidden_layer_sizes\" : [(100,), (200,),(100,50),(200,50),(200,100),(100,100,50)],\n",
    "            \"solver\" : ['sgd'],\n",
    "            \"learning_rate\" : ['constant','invscaling'],\n",
    "            \"max_iter\" : [200,300,500],\n",
    "            \"warm_start\" : [True],\n",
    "            \"activation\" :['tanh', 'relu']\n",
    "        }\n",
    "        return self.random_CV(mlp_clf,X,y,param_dist,6,3)\n",
    "    \n",
    "    def train_clf(self,clf,params,X,y):\n",
    "        clf.set_params(**params)\n",
    "        clf.fit(X,y)\n",
    "        print(\"Complete Training Accuracy\")\n",
    "        print(clf.score(X,y))\n",
    "        return clf\n",
    "        \n",
    "        \n",
    "    def start(self):\n",
    "        print(\"******Classification of Adult Data Set Begins ******\")\n",
    "        file = 'adult.data'\n",
    "        f = open(file,\"r\")\n",
    "        c = StringIO(f.read())\n",
    "        print(\"READING TRAIN DATA\")\n",
    "        # Ignoring Race and Sex Attributes\n",
    "        \n",
    "        #X_inp = np.loadtxt(c, delimiter = \",\",usecols =(0,1,2,3,4,5,6,7,10,11,12,13,14),dtype = {'names':('age','workclass','fnlwgt',\n",
    "        #                                                                                                    'education','education-num',\n",
    "        #                                                                                                    'marital-status','occupation',\n",
    "        #                                                                                                   'relationship','capital-gain',\n",
    "        #                                                                                                    'capital-loss','hours-per-week',\n",
    "        #                                                                                                    'native-country'),\n",
    "        #                                                                                           'formats':(np.float,'|S25',np.float,\n",
    "         #                                                                                                    '|S25', np.float, '|S25',\n",
    "         #                                                                                                    '|S25', '|S25', np.float,\n",
    "          #                                                                                                   np.float,np.float,'|S25')})\n",
    "        X_string  = np.char.strip(np.genfromtxt(c,dtype='str',delimiter = ',',usecols = (1,3,5,6,7,13,14)))\n",
    "        X_float = np.loadtxt(file,delimiter = \",\",usecols = (0,2,4,10,11,12), dtype = np.float).astype(int)\n",
    "        \n",
    "        print(\"Pre Processing Train data with One Hot Encoding\")\n",
    "        (X,y) = self.preprocess_data_OH(X_string[:,:-1],X_float,X_string[:,-1])\n",
    "        \n",
    "        \n",
    "        print(\"Normalizing data with Standard Scaler\")\n",
    "        (X,scaler) = self.scale_data(X)\n",
    "        \n",
    "        #Reducing dimensions to consider first 50 Principle Components based on explained_variance_ratio scores\n",
    "        print(\"Reducing dimensionality to improve classifier run time\")\n",
    "        pca = PCA(n_components = 50)\n",
    "        X_rd = pca.fit_transform(X)\n",
    "        \n",
    "        #print(X_rd[0:5])\n",
    "        #print(pca.explained_variance_ratio_)\n",
    "        \n",
    "        print(\"-- Training KNN --\")\n",
    "        knn_clf = self.train_clf(KNeighborsClassifier(),self.KNN(X_rd,y),X_rd,y) #used PCA reduced data\n",
    "        \n",
    "        #Calling SVM Classifier using unreduced data dimensions\n",
    "        print(\"--Training SVM \")\n",
    "        svm_clf = self.train_clf(svm.LinearSVC(),self.SVM_clf(X,y),X,y)\n",
    "        \n",
    "        print(\" Preprocessing with Ordinal Encoding for Decison Tree Algorithms\")\n",
    "        \n",
    "        (X_oe,y_oe) = self.preprocess_data_OE(X_string[:,:-1],X_float,X_string[:,-1]) # usine Ordinal Encoding for Decision tree algorithms\n",
    "        \n",
    "        print(\"--Training Decision Trees--\")\n",
    "        dt_clf = self.train_clf(sklearn.tree.DecisionTreeClassifier(),self.DT_clf(X_oe,y_oe),X_oe,y_oe)\n",
    "        \n",
    "        print(\"--Training Random Forests--\")\n",
    "        rf_clf = self.train_clf(sklearn.ensemble.RandomForestClassifier(),self.RF_clf(X_oe,y_oe),X_oe,y_oe)\n",
    "        \n",
    "        print(\"--Training AdaBoost-- \")\n",
    "        adb_clf = self.train_clf(sklearn.ensemble.AdaBoostClassifier(),self.ADB_clf(X_oe,y_oe),X_oe,y_oe)\n",
    "        \n",
    "        print(\"--Training Logistic Reg--\")\n",
    "        lr_clf = self.train_clf(sklearn.linear_model.LogisticRegression(),self.LR_clf(X_rd,y),X_rd,y)\n",
    "        \n",
    "        print(\"--Training Naive Bayes-- \")\n",
    "        nb_clf = self.train_clf(sklearn.naive_bayes.GaussianNB(),self.NB_clf(X_rd,y),X_rd,y)\n",
    "        \n",
    "      #  print(\"--Training Neural Networks/MLP--\")\n",
    "       # mlp_clf = self.train_clf(sklearn.neural_network.MLPClassifier(),self.MLP_clf(X_rd,y),X_rd,y)\n",
    "        \n",
    "        print(\"--READING TEST DATA\")\n",
    "        \n",
    "        file = 'adult.test'\n",
    "        f = open(file,\"r\")\n",
    "        c = StringIO(f.read())\n",
    "        \n",
    "        \n",
    "        X_string  = np.char.strip(np.genfromtxt(c,dtype='str',delimiter = ',',usecols = (1,3,5,6,7,13,14),skip_header=1))\n",
    "        X_float = np.loadtxt(file,delimiter = \",\",usecols = (0,2,4,10,11,12), dtype = np.float,skiprows=1).astype(int)\n",
    "        \n",
    "        print(\"One Hot Encoding of Data\")\n",
    "        (X_test,y_test) = self.preprocess_data_OH(X_string[:,:-1],X_float,X_string[:,-1])\n",
    "        \n",
    "        X_test = scaler.transform(X_test)\n",
    "        \n",
    "        (X_test_oe,y_test_oe) = self.preprocess_data_OE(X_string[:,:-1],X_float,X_string[:,-1])\n",
    "        \n",
    "        print(\"Reducing dimensionality to improve classifier run time\")\n",
    "        pca = PCA(n_components = 50)\n",
    "        X_test_rd = pca.fit_transform(X_test)\n",
    "        \n",
    "        print(\"**Test Data Prediction Begins*\")\n",
    "        \n",
    "        print(\"Testing KNN Classifier\")\n",
    "        print(knn_clf.score(X_test_rd,y_test))\n",
    "        \n",
    "        print(\"Testing SVM Classifier\")\n",
    "        print(svm_clf.score(X_test,y_test))\n",
    "        \n",
    "        print(\"Testing Decision Trees\")\n",
    "        print(dt_clf.score(X_test_oe,y_test_oe)) #using ordinal encoding\n",
    "        \n",
    "        print(\"Testing Random Forests\")\n",
    "        print(rf_clf.score(X_test_oe,y_test_oe))\n",
    "        \n",
    "        print(\"Testing Adaboost\")\n",
    "        print(adb_clf.score(X_test_oe,y_test_oe))\n",
    "        \n",
    "        print(\"Testing Logistic Regression\")\n",
    "        print(lr_clf.score(X_test_rd,y_test))\n",
    "        \n",
    "        print(\"Testing Naive Bayes\")\n",
    "        print(nb_clf.score(X_test_rd,y_test))\n",
    "        \n",
    "       # print(\"Testing Neural Network/MLP\")\n",
    "        #print(mlp_clf.score(X_test_rd,y_test))\n",
    "        \n",
    "        return\n",
    "    \n",
    "adult = Adult()\n",
    "adult.start()\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******Classification of Credit card Default Data Set Begins ******\n"
     ]
    },
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'charmap' codec can't decode byte 0x8d in position 750: character maps to <undefined>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-23a6343735c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcredit_card_defaults\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-23a6343735c7>\u001b[0m in \u001b[0;36mstart\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[0mfile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'default of credit card clients.xls'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m         \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"READING TRAIN DATA\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Miniconda3\\lib\\encodings\\cp1252.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIncrementalDecoder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcharmap_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdecoding_table\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mStreamWriter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mCodec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mStreamWriter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'charmap' codec can't decode byte 0x8d in position 750: character maps to <undefined>"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from io import StringIO\n",
    "import scipy\n",
    "import scipy.stats               # For reciprocal distribution\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import svm\n",
    "import sklearn.tree        # For DecisionTreeClassifier class\n",
    "import sklearn.ensemble    # For RandomForestClassifier class\n",
    "import sklearn.linear_model # For Logistic Classifier\n",
    "#from sklearn.neighbors import LSHForest\n",
    "import sklearn.naive_bayes #For Naive Bayes\n",
    "import sklearn.neural_network #For MLP classifier\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)  # Ignore sklearn deprecation warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)       # Ignore sklearn deprecation warnings\n",
    "np.set_printoptions(precision=20, suppress=True)\n",
    "\n",
    "class credit_card_defaults:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def start(self):\n",
    "        print(\"******Classification of Credit card Default Data Set Begins ******\")\n",
    "        file = 'default of credit card clients.xls'\n",
    "        f = open(file,\"r\")\n",
    "        c = StringIO(f.read())\n",
    "        print(\"READING TRAIN DATA\")\n",
    "        \n",
    "        X_string  = np.char.strip(np.gen fromtxt(file,dtype='str'))#,delimiter = ',',usecols = (1,3,5,6,7,13,14)))\n",
    "            \n",
    "        print(X_string[0])\n",
    "        return\n",
    "    \n",
    "obj = credit_card_defaults()\n",
    "obj.start()\n",
    "    \n",
    "                                \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
