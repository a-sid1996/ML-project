{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.svm   \n",
    "import sklearn.preprocessing\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import OrdinalEncoder\n",
    "import sklearn.metrics  \n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree.export import export_text\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import sklearn.neural_network \n",
    "np.set_printoptions(precision=3, suppress=True) \n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import average_precision_score, make_scorer, precision_score\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Diabetic Retinopathy dataset script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    obj = Retinopathy()\n",
    "    X_train, y_train, X_test, y_test = obj.read_data()\n",
    "    X_train, X_test = obj.preprocessing(X_train, X_test)\n",
    "    print('---------SVM--------')\n",
    "    model = obj.cv_SVM(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------KNN--------')\n",
    "    model = obj.cv_Knn(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Decision Tree--------')\n",
    "    model = obj.cv_DT(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Random Forest--------')\n",
    "    model = obj.cv_RandomForest(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------AdaBoost--------')\n",
    "    model = obj.cv_adaBoost(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Logistic Regression--------')\n",
    "    model = obj.cv_logReg(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Gaussian Naive Bayes--------')\n",
    "    model = obj.cv_GNB(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Neural Network--------')\n",
    "    model = obj.cv_NN(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### German Credit Card Data set script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    obj = German_CC()\n",
    "    X_train, y_train, X_test, y_test = obj.read_data()\n",
    "    print('---------SVM--------')\n",
    "    model = obj.cv_SVM(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------KNN--------')\n",
    "    model = obj.cv_Knn(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Decision Tree--------')\n",
    "    model = obj.cv_DT(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Random Forest--------')\n",
    "    model = obj.cv_RandomForest(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------AdaBoost--------')\n",
    "    model = obj.cv_adaBoost(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Logistic Regression--------')\n",
    "    model = obj.cv_logReg(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Gaussian Naive Bayes--------')\n",
    "    model = obj.cv_GNB(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Neural Network--------')\n",
    "    model = obj.cv_NN(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Seismic Data Set Script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------SVM--------\n",
      "Fitting 3 folds for each of 18 candidates, totalling 54 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    3.3s\n",
      "[Parallel(n_jobs=-1)]: Done  54 out of  54 | elapsed:    3.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "53.8% AUPR on validation sets (average)\n",
      "Final Accuracy on test data :  0.9477756286266924\n",
      "---------KNN--------\n",
      "{'weights': 'uniform', 'n_neighbors': 19, 'leaf_size': 8, 'algorithm': 'brute'}\n",
      "Model with rank: 1\n",
      "AUPR on validation data: 0.535 (std: 0.000)\n",
      "Parameters: {'weights': 'uniform', 'n_neighbors': 19, 'leaf_size': 8, 'algorithm': 'brute'}\n",
      "\n",
      "Model with rank: 1\n",
      "AUPR on validation data: 0.535 (std: 0.000)\n",
      "Parameters: {'weights': 'uniform', 'n_neighbors': 14, 'leaf_size': 7, 'algorithm': 'brute'}\n",
      "\n",
      "Model with rank: 1\n",
      "AUPR on validation data: 0.535 (std: 0.000)\n",
      "Parameters: {'weights': 'uniform', 'n_neighbors': 14, 'leaf_size': 38, 'algorithm': 'ball_tree'}\n",
      "\n",
      "Final Accuracy on test data :  0.9477756286266924\n",
      "---------Decision Tree--------\n",
      "{'splitter': 'random', 'min_samples_split': 0.13599876543691336, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 19, 'criterion': 'entropy', 'class_weight': None}\n",
      "Model with rank: 1\n",
      "AUPR on validation data: 0.538 (std: 0.007)\n",
      "Parameters: {'splitter': 'random', 'min_samples_split': 0.13599876543691336, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 19, 'criterion': 'entropy', 'class_weight': None}\n",
      "\n",
      "Final Accuracy on test data :  0.9477756286266924\n",
      "---------Random Forest--------\n",
      "{'n_estimators': 20, 'min_samples_split': 0.33436705351072893, 'min_samples_leaf': 5, 'max_features': None, 'max_depth': 11, 'criterion': 'entropy'}\n",
      "Model with rank: 1\n",
      "AUPR on validation data: 0.535 (std: 0.000)\n",
      "Parameters: {'n_estimators': 20, 'min_samples_split': 0.33436705351072893, 'min_samples_leaf': 5, 'max_features': None, 'max_depth': 11, 'criterion': 'entropy'}\n",
      "\n",
      "Model with rank: 1\n",
      "AUPR on validation data: 0.535 (std: 0.000)\n",
      "Parameters: {'n_estimators': 230, 'min_samples_split': 0.7482347852017673, 'min_samples_leaf': 5, 'max_features': 'log2', 'max_depth': 15, 'criterion': 'gini'}\n",
      "\n",
      "Model with rank: 1\n",
      "AUPR on validation data: 0.535 (std: 0.000)\n",
      "Parameters: {'n_estimators': 60, 'min_samples_split': 0.4962040056089939, 'min_samples_leaf': 3, 'max_features': 'log2', 'max_depth': 8, 'criterion': 'entropy'}\n",
      "\n",
      "Final Accuracy on test data :  0.9477756286266924\n",
      "---------AdaBoost--------\n",
      "Fitting 3 folds for each of 99 candidates, totalling 297 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 297 out of 297 | elapsed:  1.0min finished\n",
      "C:\\Users\\arsal\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:266: UserWarning: The total space of parameters 120 is smaller than n_iter=400. Running 120 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  % (grid_size, self.n_iter, grid_size), UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters: {'n_estimators': 1}\n",
      "53.5% AUPR on validation sets (average)\n",
      "Final Accuracy on test data :  0.9477756286266924\n",
      "---------Logistic Regression--------\n",
      "{'solver': 'liblinear', 'penalty': 'l1', 'C': 0.013094622356888408}\n",
      "Model with rank: 1\n",
      "AUPR on validation data: 0.535 (std: 0.000)\n",
      "Parameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 0.013094622356888408}\n",
      "\n",
      "Model with rank: 1\n",
      "AUPR on validation data: 0.535 (std: 0.000)\n",
      "Parameters: {'solver': 'liblinear', 'penalty': 'l1', 'C': 0.008225833077084688}\n",
      "\n",
      "Final Accuracy on test data :  0.9477756286266924\n",
      "---------Gaussian Naive Bayes--------\n",
      "{'var_smoothing': 0.0034403145682760927}\n",
      "Model with rank: 1\n",
      "AUPR on validation data: 0.253 (std: 0.109)\n",
      "Parameters: {'var_smoothing': 0.0034403145682760927}\n",
      "\n",
      "Final Accuracy on test data :  0.8974854932301741\n",
      "---------Neural Network--------\n",
      "{'hidden_layer_sizes': 48, 'activation': 'relu'}\n",
      "Model with rank: 1\n",
      "AUPR on validation data: 0.541 (std: 0.009)\n",
      "Parameters: {'hidden_layer_sizes': 48, 'activation': 'relu'}\n",
      "\n",
      "Final Accuracy on test data :  0.9458413926499033\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    obj = Seismic_Bumps()\n",
    "    X_train, y_train, X_test, y_test = obj.read_data()\n",
    "    X_train, X_test = obj.preprocessing(X_train, X_test)\n",
    "    print('---------SVM--------')\n",
    "    model = obj.cv_SVM(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------KNN--------')\n",
    "    model = obj.cv_Knn(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Decision Tree--------')\n",
    "    model = obj.cv_DT(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Random Forest--------')\n",
    "    model = obj.cv_RandomForest(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------AdaBoost--------')\n",
    "    model = obj.cv_adaBoost(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Logistic Regression--------')\n",
    "    model = obj.cv_logReg(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Gaussian Naive Bayes--------')\n",
    "    model = obj.cv_GNB(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Neural Network--------')\n",
    "    model = obj.cv_NN(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Adult dataset script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For adult data set please run this cell\n",
    "adult = Adult()\n",
    "adult.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model with rank: 1\n",
      "Mean validation score: 0.858 (std: 0.027)\n",
      "Parameters: {'weights': 'uniform', 'n_neighbors': 13, 'leaf_size': 51, 'algorithm': 'kd_tree'}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.858 (std: 0.027)\n",
      "Parameters: {'weights': 'uniform', 'n_neighbors': 13, 'leaf_size': 24, 'algorithm': 'auto'}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.858 (std: 0.027)\n",
      "Parameters: {'weights': 'uniform', 'n_neighbors': 13, 'leaf_size': 37, 'algorithm': 'brute'}\n",
      "\n",
      "Score with test data 0.8476190476190476\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.865 (std: 0.014)\n",
      "Parameters: {'splitter': 'random', 'presort': False, 'min_samples_split': 0.35784939906420277, 'min_samples_leaf': 2, 'max_features': None, 'criterion': 'entropy', 'class_weight': 'balanced'}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.865 (std: 0.014)\n",
      "Parameters: {'splitter': 'random', 'presort': False, 'min_samples_split': 0.2977026870264977, 'min_samples_leaf': 4, 'max_features': None, 'criterion': 'gini', 'class_weight': 'balanced'}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.863 (std: 0.012)\n",
      "Parameters: {'splitter': 'random', 'presort': True, 'min_samples_split': 0.29621141350330427, 'min_samples_leaf': 4, 'max_features': None, 'criterion': 'gini', 'class_weight': None}\n",
      "\n",
      "Score with test data 0.8285714285714286\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.884 (std: 0.031)\n",
      "Parameters: {'warm_start': True, 'n_estimators': 200, 'min_samples_split': 0.19832667290362826, 'min_samples_leaf': 5, 'max_features': 'sqrt', 'criterion': 'entropy'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.879 (std: 0.031)\n",
      "Parameters: {'warm_start': False, 'n_estimators': 430, 'min_samples_split': 0.10617931630820576, 'min_samples_leaf': 2, 'max_features': 'auto', 'criterion': 'gini'}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.873 (std: 0.028)\n",
      "Parameters: {'warm_start': False, 'n_estimators': 70, 'min_samples_split': 0.39886991002028993, 'min_samples_leaf': 1, 'max_features': 'log2', 'criterion': 'gini'}\n",
      "\n",
      "Score with test data 0.8380952380952381\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.877 (std: 0.028)\n",
      "Parameters: {'n_estimators': 20}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.870 (std: 0.048)\n",
      "Parameters: {'n_estimators': 60}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.867 (std: 0.055)\n",
      "Parameters: {'n_estimators': 180}\n",
      "\n",
      "Score with test data 0.8476190476190476\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.880 (std: 0.022)\n",
      "Parameters: {'warm_start': False, 'solver': 'liblinear', 'penalty': 'l2', 'multi_class': 'ovr', 'fit_intercept': False, 'C': 0.05077593205901765}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.880 (std: 0.022)\n",
      "Parameters: {'warm_start': False, 'solver': 'liblinear', 'penalty': 'l2', 'multi_class': 'auto', 'fit_intercept': False, 'C': 0.05077593205901765}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.880 (std: 0.035)\n",
      "Parameters: {'warm_start': True, 'solver': 'liblinear', 'penalty': 'l2', 'multi_class': 'ovr', 'fit_intercept': True, 'C': 0.04580254927769545}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.880 (std: 0.035)\n",
      "Parameters: {'warm_start': True, 'solver': 'liblinear', 'penalty': 'l2', 'multi_class': 'auto', 'fit_intercept': True, 'C': 0.04580254927769545}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.880 (std: 0.035)\n",
      "Parameters: {'warm_start': False, 'solver': 'liblinear', 'penalty': 'l2', 'multi_class': 'ovr', 'fit_intercept': True, 'C': 0.04580254927769545}\n",
      "\n",
      "Score with test data 0.8380952380952381\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.827 (std: 0.036)\n",
      "Parameters: {'var_smoothing': 0.12811573097627071}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.827 (std: 0.036)\n",
      "Parameters: {'var_smoothing': 0.16524954449957474}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.827 (std: 0.036)\n",
      "Parameters: {'var_smoothing': 0.12911626339891857}\n",
      "\n",
      "Score with test data 0.819047619047619\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.892 (std: 0.042)\n",
      "Parameters: {'warm_start': True, 'solver': 'adam', 'hidden_layer_sizes': 96, 'activation': 'relu'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.884 (std: 0.039)\n",
      "Parameters: {'warm_start': False, 'solver': 'adam', 'hidden_layer_sizes': 88, 'activation': 'relu'}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.882 (std: 0.030)\n",
      "Parameters: {'warm_start': False, 'solver': 'adam', 'hidden_layer_sizes': 30, 'activation': 'relu'}\n",
      "\n",
      "Score with test data 0.8380952380952381\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.848 (std: 0.014)\n",
      "Parameters: {'shrinking': False, 'kernel': 'linear', 'gamma': 'auto', 'degree': 3, 'decision_function_shape': 'ovr', 'coef0': 9.814116111487385}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.848 (std: 0.014)\n",
      "Parameters: {'shrinking': False, 'kernel': 'linear', 'gamma': 'auto', 'degree': 4, 'decision_function_shape': 'ovo', 'coef0': 0.48131129814336204}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.713 (std: 0.027)\n",
      "Parameters: {'shrinking': True, 'kernel': 'sigmoid', 'gamma': 'auto', 'degree': 5, 'decision_function_shape': 'ovr', 'coef0': 3.1386810008865353}\n",
      "\n",
      "Score with test data 0.8476190476190476\n"
     ]
    }
   ],
   "source": [
    "stat = statlog_aus()\n",
    "stat.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1941, 34)\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.987 (std: 0.007)\n",
      "Parameters: {'weights': 'distance', 'n_neighbors': 5, 'leaf_size': 32, 'algorithm': 'kd_tree'}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.987 (std: 0.007)\n",
      "Parameters: {'weights': 'distance', 'n_neighbors': 5, 'leaf_size': 34, 'algorithm': 'kd_tree'}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.987 (std: 0.007)\n",
      "Parameters: {'weights': 'distance', 'n_neighbors': 5, 'leaf_size': 18, 'algorithm': 'brute'}\n",
      "\n",
      "Score with test data 0.9896551724137931\n",
      "Model with rank: 1\n",
      "Mean validation score: 1.000 (std: 0.000)\n",
      "Parameters: {'splitter': 'random', 'presort': False, 'min_samples_split': 0.13497904569300334, 'min_samples_leaf': 3, 'max_features': None, 'criterion': 'entropy', 'class_weight': None}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 1.000 (std: 0.000)\n",
      "Parameters: {'splitter': 'best', 'presort': True, 'min_samples_split': 0.2403328343104869, 'min_samples_leaf': 3, 'max_features': None, 'criterion': 'gini', 'class_weight': None}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 1.000 (std: 0.000)\n",
      "Parameters: {'splitter': 'best', 'presort': True, 'min_samples_split': 0.3322713617719316, 'min_samples_leaf': 2, 'max_features': None, 'criterion': 'gini', 'class_weight': 'balanced'}\n",
      "\n",
      "Score with test data 1.0\n",
      "Model with rank: 1\n",
      "Mean validation score: 1.000 (std: 0.000)\n",
      "Parameters: {'warm_start': False, 'n_estimators': 220, 'min_samples_split': 0.02339375606658245, 'min_samples_leaf': 5, 'max_features': None, 'criterion': 'gini'}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.996 (std: 0.002)\n",
      "Parameters: {'warm_start': False, 'n_estimators': 30, 'min_samples_split': 0.013635287977327826, 'min_samples_leaf': 1, 'max_features': 'log2', 'criterion': 'entropy'}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.993 (std: 0.002)\n",
      "Parameters: {'warm_start': False, 'n_estimators': 360, 'min_samples_split': 0.04414792977694326, 'min_samples_leaf': 3, 'max_features': 'log2', 'criterion': 'entropy'}\n",
      "\n",
      "Score with test data 1.0\n",
      "Model with rank: 1\n",
      "Mean validation score: 1.000 (std: 0.000)\n",
      "Parameters: {'n_estimators': 280}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 1.000 (std: 0.000)\n",
      "Parameters: {'n_estimators': 120}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 1.000 (std: 0.000)\n",
      "Parameters: {'n_estimators': 60}\n",
      "\n",
      "Score with test data 1.0\n",
      "Model with rank: 1\n",
      "Mean validation score: 1.000 (std: 0.000)\n",
      "Parameters: {'warm_start': True, 'solver': 'liblinear', 'penalty': 'l1', 'multi_class': 'auto', 'fit_intercept': False, 'C': 0.2226396957174417}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 1.000 (std: 0.000)\n",
      "Parameters: {'warm_start': False, 'solver': 'liblinear', 'penalty': 'l1', 'multi_class': 'auto', 'fit_intercept': True, 'C': 0.2800646656325383}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 1.000 (std: 0.000)\n",
      "Parameters: {'warm_start': False, 'solver': 'liblinear', 'penalty': 'l1', 'multi_class': 'ovr', 'fit_intercept': True, 'C': 0.5988883673578533}\n",
      "\n",
      "Score with test data 1.0\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.996 (std: 0.004)\n",
      "Parameters: {'var_smoothing': 0.0058211927949236575}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.995 (std: 0.003)\n",
      "Parameters: {'var_smoothing': 0.012003864696654154}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.994 (std: 0.003)\n",
      "Parameters: {'var_smoothing': 0.025248853035547136}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.994 (std: 0.003)\n",
      "Parameters: {'var_smoothing': 0.03905186646492542}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.994 (std: 0.003)\n",
      "Parameters: {'var_smoothing': 0.03952704448363309}\n",
      "\n",
      "Score with test data 1.0\n",
      "Model with rank: 1\n",
      "Mean validation score: 1.000 (std: 0.000)\n",
      "Parameters: {'warm_start': True, 'solver': 'lbfgs', 'hidden_layer_sizes': 63, 'activation': 'tanh'}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 1.000 (std: 0.000)\n",
      "Parameters: {'warm_start': True, 'solver': 'lbfgs', 'hidden_layer_sizes': 179, 'activation': 'identity'}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.999 (std: 0.001)\n",
      "Parameters: {'warm_start': False, 'solver': 'sgd', 'hidden_layer_sizes': 184, 'activation': 'tanh'}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.999 (std: 0.001)\n",
      "Parameters: {'warm_start': False, 'solver': 'adam', 'hidden_layer_sizes': 128, 'activation': 'identity'}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.999 (std: 0.001)\n",
      "Parameters: {'warm_start': True, 'solver': 'lbfgs', 'hidden_layer_sizes': 16, 'activation': 'relu'}\n",
      "\n",
      "Score with test data 1.0\n",
      "Model with rank: 1\n",
      "Mean validation score: 1.000 (std: 0.000)\n",
      "Parameters: {'shrinking': True, 'kernel': 'linear', 'gamma': 'scale', 'degree': 1, 'decision_function_shape': 'ovr', 'coef0': 3.6984054478005577}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 1.000 (std: 0.000)\n",
      "Parameters: {'shrinking': False, 'kernel': 'linear', 'gamma': 'scale', 'degree': 4, 'decision_function_shape': 'ovo', 'coef0': 3.1157337629364656}\n",
      "\n",
      "Model with rank: 1\n",
      "Mean validation score: 1.000 (std: 0.000)\n",
      "Parameters: {'shrinking': True, 'kernel': 'linear', 'gamma': 'scale', 'degree': 1, 'decision_function_shape': 'ovo', 'coef0': 8.634079952134513}\n",
      "\n",
      "Score with test data 1.0\n"
     ]
    }
   ],
   "source": [
    "stee = steel()\n",
    "stee.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Breast Cancer Dataset script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    obj = Breast_Cancer()\n",
    "    X_train, y_train, X_test, y_test = obj.read_data()\n",
    "    X_train, X_test = obj.preprocessing(X_train, X_test)\n",
    "    print('---------SVM--------')\n",
    "    model = obj.cv_SVM(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------KNN--------')\n",
    "    model = obj.cv_Knn(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Decision Tree--------')\n",
    "    model = obj.cv_DT(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Random Forest--------')\n",
    "    model = obj.cv_RandomForest(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------AdaBoost--------')\n",
    "    model = obj.cv_adaBoost(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Logistic Regression--------')\n",
    "    model = obj.cv_logReg(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Gaussian Naive Bayes--------')\n",
    "    model = obj.cv_GNB(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)\n",
    "    print('---------Neural Network--------')\n",
    "    model = obj.cv_NN(X_train, y_train)\n",
    "    obj.predict(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Breast_Cancer:\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def read_data(self):\n",
    "        X = np.loadtxt('wdbc.data', delimiter=',', usecols=range(2,32))\n",
    "        y = np.loadtxt('wdbc.data', delimiter=',', usecols=[1], dtype= '|S1').astype(str)\n",
    "        le = preprocessing.LabelEncoder()\n",
    "        # B = 0, M = 1\n",
    "        y = le.fit_transform(y)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "        return X_train, y_train, X_test, y_test\n",
    "    \n",
    "    def preprocessing(self, X_train, X_test):\n",
    "        # preprocessing using standard scaler\n",
    "        scaler = sklearn.preprocessing.StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        return X_train, X_test\n",
    "        \n",
    "    def cv_SVM(self, X, y):\n",
    "        C_grid = [0.1, 1, 10]\n",
    "        gamma_grid = np.logspace(-2, 1, 4)[0:3]\n",
    "        svm = sklearn.svm.SVC(kernel='poly')\n",
    "        param_grid = { 'C' : C_grid, 'gamma' : gamma_grid, \"kernel\" : ['poly', 'rbf', 'sigmoid'], }\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(svm, param_grid, verbose=1, cv=3)\n",
    "        gridcv.fit(X_train, y_train)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))        \n",
    "        return gridcv.best_estimator_\n",
    "\n",
    "    def cv_Knn(self, X, y):\n",
    "        neigh = KNeighborsClassifier()\n",
    "        param_grid = {\n",
    "            \"n_neighbors\" : np.arange(1,20),\n",
    "            \"algorithm\" : ['ball_tree', 'kd_tree', 'brute'],\n",
    "            \"weights\" : ['uniform', 'distance'],\n",
    "            \"leaf_size\" : np.arange(1,60)\n",
    "        }\n",
    "        return Breast_Cancer.randomCV(neigh, X, y, param_grid, 400, 6)\n",
    "    \n",
    "    def cv_DT(self, X, y):\n",
    "        dt = tree.DecisionTreeClassifier()\n",
    "        param_grid = {\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"splitter\" : ['best', 'random'],\n",
    "            \"min_samples_split\" : np.random.random_sample((100,)),\n",
    "            \"max_features\" : ['auto', 'sqrt', 'log2', None],\n",
    "            \"class_weight\" : [None, 'balanced'],\n",
    "            \"presort\" : [True, False],\n",
    "            \"min_samples_leaf\" : np.arange(1,6),\n",
    "            'max_depth': range(1, 20)\n",
    "        }\n",
    "        return Breast_Cancer.randomCV(dt, X, y, param_grid, 400, 6)\n",
    "        \n",
    "    def cv_RandomForest(self, X, y):\n",
    "        rf = RandomForestClassifier()\n",
    "        param_grid = {\n",
    "            \"n_estimators\" : [10*x for x in np.arange(1,50)],\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"min_samples_split\" : np.random.random_sample((100,)),\n",
    "            \"max_features\" : ['auto', 'sqrt', 'log2', None],\n",
    "            #\"class_weight\" : [None, 'balanced'],\n",
    "            \"min_samples_leaf\" : np.arange(1,6),\n",
    "            #\"bootstrap\" : [True, False],\n",
    "            #\"oob_score\" : [True, False],\n",
    "            \"warm_start\" : [True, False],\n",
    "        }\n",
    "        return Breast_Cancer.randomCV(rf, X, y, param_grid, 40, 6)\n",
    "        \n",
    "        \n",
    "    def cv_adaBoost(self, X, y):\n",
    "        ada_boost = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "        param_grid = {'n_estimators': range(1, 100)}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(ada_boost, param_grid, verbose=1, cv=3)\n",
    "        gridcv.fit(X, y)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "        \n",
    "    def cv_logReg(self, X, y):\n",
    "        lr = LogisticRegression()\n",
    "        param_grid = {\n",
    "            \"penalty\" : ['l1', 'l2'],\n",
    "            #\"dual\" : [True, False],\n",
    "            \"C\" : np.random.rand(60),\n",
    "            \"fit_intercept\" : [True, False],\n",
    "            \"warm_start\" : [True, False],\n",
    "            \"multi_class\" : ['ovr', 'auto'],\n",
    "            \"solver\" : [ 'liblinear']\n",
    "        }\n",
    "        return Breast_Cancer.randomCV(lr, X, y, param_grid, 400, 6)\n",
    "        \n",
    "    def cv_GNB(self, X, y):\n",
    "        gnb = GaussianNB()\n",
    "        param_grid = {\n",
    "            \"var_smoothing\" : np.random.random_sample((100,))\n",
    "        }\n",
    "        return Breast_Cancer.randomCV(gnb, X, y, param_grid, 100, 6)\n",
    "    \n",
    "    def cv_NN(self, X, y):\n",
    "        nn = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(50,),\n",
    "                                           solver='sgd', batch_size=100, max_iter=10,\n",
    "                                           learning_rate_init=.01, momentum=0.9, alpha=0.05,\n",
    "                                           verbose=False, random_state=0)\n",
    "\n",
    "        param_grid ={\n",
    "                    'hidden_layer_sizes' : range(2,100),\n",
    "                    \"activation\" : ['identity', 'logistic', 'tanh', 'relu']\n",
    "                    }\n",
    "        return Breast_Cancer.randomCV(nn, X, y, param_grid, 200, 6)\n",
    "        \n",
    "    def randomCV(clf, X, y, param_grid, n_iter, cv):\n",
    "        random_search = RandomizedSearchCV(clf, param_distributions = param_grid, n_iter = n_iter, cv = cv, iid = False)\n",
    "        random_search.fit(X, y)\n",
    "        Breast_Cancer.report(random_search.cv_results_)\n",
    "        return random_search.best_estimator_\n",
    "    \n",
    "    def report(results, n_top=3):\n",
    "        for i in range(1, n_top + 1):\n",
    "            candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "            k = 0 \n",
    "            for candidate in candidates:\n",
    "                print(\"Model with rank: {0}\".format(i))\n",
    "                print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                      results['mean_test_score'][candidate],\n",
    "                      results['std_test_score'][candidate]))\n",
    "                print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "                print(\"\")\n",
    "                k += 1\n",
    "                if k ==3:\n",
    "                    break\n",
    "    def predict(self, model, X_test, y_test):        \n",
    "        predict = model.predict(X_test)\n",
    "        accuracy = sklearn.metrics.accuracy_score(y_test, predict)\n",
    "        print(accuracy)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from io import StringIO\n",
    "import scipy\n",
    "import scipy.stats             \n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn import svm\n",
    "import sklearn.tree        # For DecisionTreeClassifier class\n",
    "import sklearn.ensemble    # For RandomForestClassifier class\n",
    "import sklearn.linear_model # For Logistic Classifier\n",
    "#from sklearn.neighbors import LSHForest\n",
    "import sklearn.naive_bayes #For Naive Bayes\n",
    "import sklearn.neural_network #For MLP classifier\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "np.set_printoptions(precision=20, suppress=True)\n",
    "\n",
    "class Adult:\n",
    "    #Removing Race and Sex Variable form data for fairness purpose\n",
    "    def preprocess_data_OH(self,X,X_num,y):\n",
    "        X= X.tolist()\n",
    "        #print(X[0])\n",
    "        workclass = ['Private','Self-emp-not-inc','Self-emp-inc','Federal-gov','Local-gov','State-gov','Without-pay','Never-worked']\n",
    "        education = ['Bachelors','Some-college','11th','HS-grad','Prof-school','Assoc-acdm','Assoc-voc','9th','7th-8th','12th','Masters','1st-4th','10th','Doctorate','5th-6th','Preschool']\n",
    "        marital_status = ['Married-civ-spouse','Divorced','Never-married','Separated','Widowed','Married-spouse-absent','Married-AF-spouse']\n",
    "        occupation = ['Tech-support','Craft-repair','Other-service','Sales','Exec-managerial','Prof-specialty','Handlers-cleaners','Machine-op-inspct','Adm-clerical','Farming-fishing','Transport-moving','Priv-house-serv','Protective-serv','Armed-Forces']\n",
    "        relationship = ['Wife','Own-child','Husband','Not-in-family','Other-relative','Unmarried']\n",
    "        native_country = ['United-States','Cambodia','England','Puerto-Rico','Canada','Germany','Outlying-US(Guam-USVI-etc)','India','Japan','Greece','South','China','Cuba','Iran','Honduras','Philippines','Italy','Poland','Jamaica','Vietnam','Mexico','Portugal','Ireland','France','Dominican-Republic','Laos','Ecuador','Taiwan','Haiti','Columbia','Hungary','Guatemala','Nicaragua','Scotland','Thailand','Yugoslavia','El-Salvador','Trinadad&Tobago','Peru','Hong','Holand-Netherlands']\n",
    "        \n",
    "        encoder = preprocessing.OneHotEncoder(categories=[workclass, education, marital_status,occupation,relationship,native_country],\n",
    "                                              handle_unknown='ignore',sparse=False)\n",
    "        print(encoder.fit(X))\n",
    "        X = encoder.transform(X)\n",
    "        X = np.column_stack((X_num,X)) #Combine numeric and encoded string input data\n",
    "        y[y=='<=50K']= 0\n",
    "        y[y=='>50K'] = 1\n",
    "        y[y=='<=50K.']= 0\n",
    "        y[y=='>50K.'] = 1\n",
    "        y = y.astype(int)\n",
    "        return (X,y)\n",
    "    \n",
    "    def scale_data(self,X):\n",
    "        scaler = preprocessing.StandardScaler(with_mean = False).fit(X)\n",
    "        X = scaler.transform(X)\n",
    "        return(X,scaler)\n",
    "    \n",
    "    def preprocess_data_OE(self,X,X_num,y):\n",
    "        X = X.tolist()\n",
    "        encoder = preprocessing.OrdinalEncoder()\n",
    "        encoder.fit(X)\n",
    "        X = encoder.transform(X)\n",
    "        X = np.column_stack((X_num,X))\n",
    "        y[y=='<=50K.']= 0\n",
    "        y[y=='<=50K.']= 0\n",
    "        y[y=='>50K.'] = 1\n",
    "        y[y=='<=50K.']= 0\n",
    "        y = y.astype(int)\n",
    "        return(X,y)\n",
    "    \n",
    "    def __init__(self):\n",
    "        #write your actual run code\n",
    "        pass\n",
    "    def random_CV(self,clf,X,y,param_grid,n_iter,cv):\n",
    "        scorer_AUROC = make_scorer(sklearn.metrics.roc_auc_score) #using ROC for scoring criteria\n",
    "        print(\"Starting search\")\n",
    "        print(\"Score mechanism implemented by RandomizedCV - AUROC score\")\n",
    "        random_search = model_selection.RandomizedSearchCV(clf, param_distributions = param_grid,n_iter = n_iter, cv = cv,\n",
    "                                           iid = False,verbose=1,scoring = scorer_AUROC,n_jobs = 4)\n",
    "        random_search.fit(X, y)\n",
    "        print(\"best parameters:\", random_search.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (random_search.best_score_*100))\n",
    "        return random_search.best_params_\n",
    "    \n",
    "    def KNN(self,X,y):\n",
    "        print(\"Starting KNN classification- Expected to take about 5 mins as its a hude data set\")\n",
    "              \n",
    "        knn_clf = KNeighborsClassifier()\n",
    "        param_dist = {'n_neighbors': range(1,100),\n",
    "                     \"algorithm\" : ['ball_tree', 'kd_tree'],\n",
    "                    \"weights\" : ['uniform', 'distance'],\n",
    "                    \"leaf_size\" : range(1,100)}\n",
    "        print(\"Calling random_cv\")\n",
    "        return self.random_CV(knn_clf,X,y,param_dist,4,3)\n",
    "        \n",
    "        \n",
    "    def SVM_clf(self,X,y):\n",
    "        print(\"Starting SVM classification\")        \n",
    "        svm_clf = svm.LinearSVC()\n",
    "        param_dist = {\n",
    "            'C'     : scipy.stats.reciprocal(1.0, 1000.),        \n",
    "            'dual': [True,False],\n",
    "            'max_iter' : np.arange(2000,10000,200)\n",
    "            }\n",
    "        return self.random_CV(svm_clf,X,y,param_dist,10,3)\n",
    "        \n",
    "    def DT_clf(self,X,y):\n",
    "        tree_clf = sklearn.tree.DecisionTreeClassifier()\n",
    "        param_dist = {\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"splitter\" : ['best', 'random'],\n",
    "            \"max_depth\" :[None,500,750,1000,1500,2000],\n",
    "            \"max_features\": [\"sqrt\",\"log2\",None]\n",
    "        }\n",
    "        return self.random_CV(tree_clf,X,y,param_dist,15,5)\n",
    "    \n",
    "    def RF_clf(self,X,y):\n",
    "        print(\"Random Forest Classifier Called\")\n",
    "        rf_clf = sklearn.ensemble.RandomForestClassifier()\n",
    "        param_dist = {\n",
    "            \"n_estimators\" : [10,25,50,75,100,125,150,175,200],\n",
    "            \"max_depth\" :[None,500,750,1000,1500,2000],\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"max_features\": [\"sqrt\",\"log2\",None],\n",
    "            \"n_jobs\": [4],\n",
    "            \"warm_start\" : [True]\n",
    "           # \"bootstrap\": [True,False]\n",
    "        }\n",
    "        return self.random_CV(rf_clf,X,y,param_dist,12,3)\n",
    "        \n",
    "        \n",
    "    def ADB_clf(self,X,y):\n",
    "        print(\"AdaBoost Classifier Called\")\n",
    "        adb_clf = sklearn.ensemble.AdaBoostClassifier()\n",
    "        param_dist = {\n",
    "            \"n_estimators\" : [50,100,150,200,300,500,750,1000],\n",
    "            \"algorithm\" : ['SAMME', 'SAMME.R'],\n",
    "            \"random_state\" : [0]\n",
    "        }\n",
    "        return self.random_CV(adb_clf,X,y,param_dist,5,5)\n",
    "        \n",
    "    \n",
    "    def LR_clf(self,X,y):\n",
    "        print(\"Logistic Reg Classifier Called\")\n",
    "        lr_clf = sklearn.linear_model.LogisticRegression()\n",
    "        param_dist = {\n",
    "            \"fit_intercept\" : [True, False],\n",
    "            'C'     : scipy.stats.reciprocal(1.0, 1000.),\n",
    "            \"solver\" : ['lbfgs','sag','saga','newton-cg'],\n",
    "            \"penalty\" : ['l2'],\n",
    "            'max_iter' : np.arange(500,10000,500)\n",
    "        }\n",
    "        return self.random_CV(lr_clf,X,y,param_dist,15,3)\n",
    "        \n",
    "    \n",
    "    def NB_clf(self,X,y):\n",
    "        print(\"Naive Bayes Classifier called\")\n",
    "        nb_clf = sklearn.naive_bayes.GaussianNB()\n",
    "        param_dist = {\n",
    "            \"priors\": [None,[0.5,0.5],[0.6,0.4],[0.4,0.6],[0.3,0.7],[0.7,0.3]]\n",
    "        }\n",
    "        return self.random_CV(nb_clf,X,y,param_dist,5,5)\n",
    "\n",
    "    \n",
    "    def MLP_clf(self,X,y):\n",
    "        print(\"Neural Network / MLP Classifier called\")\n",
    "        mlp_clf = sklearn.neural_network.MLPClassifier()\n",
    "        param_dist = {\n",
    "            \"hidden_layer_sizes\" : [(100,), (200,),(100,50),(200,50),(200,100),(100,100,50)],\n",
    "            \"solver\" : ['sgd'],\n",
    "            \"learning_rate\" : ['constant','invscaling'],\n",
    "            \"max_iter\" : [200,300,500],\n",
    "            \"warm_start\" : [True],\n",
    "            \"activation\" :['tanh', 'relu']\n",
    "        }\n",
    "        return self.random_CV(mlp_clf,X,y,param_dist,6,3)\n",
    "    \n",
    "    def train_clf(self,clf,params,X,y):\n",
    "        clf.set_params(**params)\n",
    "        clf.fit(X,y)\n",
    "        print(\"Complete Training Accuracy\")\n",
    "        print(clf.score(X,y))\n",
    "        return clf\n",
    "        \n",
    "        \n",
    "    def start(self):\n",
    "        print(\"******Classification of Adult Data Set Begins ******\")\n",
    "        file = 'adult.data'\n",
    "        f = open(file,\"r\")\n",
    "        c = StringIO(f.read())\n",
    "        print(\"READING TRAIN DATA\")\n",
    "        \n",
    "        # Ignoring Race and Sex Attributes       \n",
    "        X_string  = np.char.strip(np.genfromtxt(c,dtype='str',delimiter = ',',usecols = (1,3,5,6,7,13,14)))\n",
    "        X_float = np.loadtxt(file,delimiter = \",\",usecols = (0,2,4,10,11,12), dtype = np.float).astype(int)\n",
    "        \n",
    "        print(\"Pre Processing Train data with One Hot Encoding\")\n",
    "        (X,y) = self.preprocess_data_OH(X_string[:,:-1],X_float,X_string[:,-1])\n",
    "        \n",
    "        \n",
    "        print(\"Normalizing data with Standard Scaler\")\n",
    "        (X,scaler) = self.scale_data(X)\n",
    "        \n",
    "        #Reducing dimensions to consider first 50 Principle Components based on explained_variance_ratio scores\n",
    "        print(\"Reducing dimensionality to improve classifier run time\")\n",
    "        pca = PCA(n_components = 50)\n",
    "        X_rd = pca.fit_transform(X)\n",
    "                \n",
    "        print(\"-- Training KNN --\")\n",
    "        knn_clf = self.train_clf(KNeighborsClassifier(),self.KNN(X_rd,y),X_rd,y) #used PCA reduced data\n",
    "        \n",
    "        #Calling SVM Classifier using unreduced data dimensions\n",
    "        print(\"--Training SVM \")\n",
    "        svm_clf = self.train_clf(svm.LinearSVC(),self.SVM_clf(X,y),X,y)\n",
    "        \n",
    "        print(\" Preprocessing with Ordinal Encoding for Decison Tree Algorithms\")\n",
    "        \n",
    "        (X_oe,y_oe) = self.preprocess_data_OE(X_string[:,:-1],X_float,X_string[:,-1]) # usine Ordinal Encoding for Decision tree algorithms\n",
    "        \n",
    "        print(\"--Training Decision Trees--\")\n",
    "        dt_clf = self.train_clf(sklearn.tree.DecisionTreeClassifier(),self.DT_clf(X_oe,y_oe),X_oe,y_oe)\n",
    "        \n",
    "        print(\"--Training Random Forests--\")\n",
    "        rf_clf = self.train_clf(sklearn.ensemble.RandomForestClassifier(),self.RF_clf(X_oe,y_oe),X_oe,y_oe)\n",
    "        \n",
    "        print(\"--Training AdaBoost-- \")\n",
    "        adb_clf = self.train_clf(sklearn.ensemble.AdaBoostClassifier(),self.ADB_clf(X_oe,y_oe),X_oe,y_oe)\n",
    "        \n",
    "        print(\"--Training Logistic Reg--\")\n",
    "        lr_clf = self.train_clf(sklearn.linear_model.LogisticRegression(),self.LR_clf(X_rd,y),X_rd,y)\n",
    "        \n",
    "        print(\"--Training Naive Bayes-- \")\n",
    "        nb_clf = self.train_clf(sklearn.naive_bayes.GaussianNB(),self.NB_clf(X_rd,y),X_rd,y)\n",
    "        \n",
    "        print(\"--READING TEST DATA\")\n",
    "        \n",
    "        file = 'adult.test'\n",
    "        f = open(file,\"r\")\n",
    "        c = StringIO(f.read())\n",
    "        \n",
    "        X_string  = np.char.strip(np.genfromtxt(c,dtype='str',delimiter = ',',usecols = (1,3,5,6,7,13,14),skip_header=1))\n",
    "        X_float = np.loadtxt(file,delimiter = \",\",usecols = (0,2,4,10,11,12), dtype = np.float,skiprows=1).astype(int)\n",
    "        \n",
    "        print(\"One Hot Encoding of Data\")\n",
    "        (X_test,y_test) = self.preprocess_data_OH(X_string[:,:-1],X_float,X_string[:,-1])        \n",
    "        X_test = scaler.transform(X_test)        \n",
    "        (X_test_oe,y_test_oe) = self.preprocess_data_OE(X_string[:,:-1],X_float,X_string[:,-1])\n",
    "        \n",
    "        print(\"Reducing dimensionality to improve classifier run time\")\n",
    "        pca = PCA(n_components = 50)\n",
    "        X_test_rd = pca.fit_transform(X_test)\n",
    "        \n",
    "        print(\"**Test Data Prediction Begins*\")\n",
    "        \n",
    "        print(\"Testing KNN Classifier\")\n",
    "        print(knn_clf.score(X_test_rd,y_test))\n",
    "        \n",
    "        print(\"Testing SVM Classifier\")\n",
    "        print(svm_clf.score(X_test,y_test))\n",
    "        \n",
    "        print(\"Testing Decision Trees\")\n",
    "        print(dt_clf.score(X_test_oe,y_test_oe)) #using ordinal encoding\n",
    "        \n",
    "        print(\"Testing Random Forests\")\n",
    "        print(rf_clf.score(X_test_oe,y_test_oe))\n",
    "        \n",
    "        print(\"Testing Adaboost\")\n",
    "        print(adb_clf.score(X_test_oe,y_test_oe))\n",
    "        \n",
    "        print(\"Testing Logistic Regression\")\n",
    "        print(lr_clf.score(X_test_rd,y_test))\n",
    "        \n",
    "        print(\"Testing Naive Bayes\")\n",
    "        print(nb_clf.score(X_test_rd,y_test))        \n",
    "        \n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Seismic_Bumps:\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def read_data(self):\n",
    "        # reading data and doing 80-20 split\n",
    "        x1 = np.loadtxt('seismic-bumps.csv', delimiter=',', skiprows=1, usecols=(3,4,5,6,8,9,10,11,12,13,14,15,16,17))\n",
    "        x2 = np.loadtxt('seismic-bumps.csv', delimiter=',', skiprows=1, usecols=[0,1,2,7], dtype= '|S1').astype(str)\n",
    "        encoder = OrdinalEncoder()\n",
    "        x2 = encoder.fit_transform(x2)\n",
    "        X = np.column_stack((x1,x2))\n",
    "        y = np.loadtxt('seismic-bumps.csv', delimiter=',', skiprows=1, usecols=(18))\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "        return X_train, y_train, X_test, y_test\n",
    "    \n",
    "    def preprocessing(self, X_train, X_test):\n",
    "        # preprocessing using standard scaler\n",
    "        scaler = sklearn.preprocessing.StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_train = scaler.transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        return X_train, X_test\n",
    "    \n",
    "    # Scorer for calculating area under P and R curve\n",
    "    def score_func(y_true, y_pred):\n",
    "        precision, recall, threshold = metrics.precision_recall_curve(y_true, y_pred)\n",
    "        return metrics.auc(recall, precision)\n",
    "        \n",
    "    def cv_SVM(self, X, y):        \n",
    "        scorer = make_scorer(Seismic_Bumps.score_func)        \n",
    "        C_grid = [0.1, 1, 10]\n",
    "        gamma_grid = np.logspace(-2, 1, 4)[0:3]\n",
    "        svm = sklearn.svm.SVC(kernel='rbf')\n",
    "        param_grid = { 'C' : C_grid, 'gamma' : gamma_grid, 'kernel' : ['rbf', 'sigmoid']}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(svm, param_grid, n_jobs=-1, verbose=1, cv=3, scoring = scorer, refit=True)\n",
    "        gridcv.fit(X_train, y_train)        \n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% AUPR on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "\n",
    "    def cv_Knn(self, X, y):\n",
    "        neigh = KNeighborsClassifier()\n",
    "        param_grid = {\n",
    "            \"n_neighbors\" : np.arange(1,20),\n",
    "            \"algorithm\" : ['ball_tree', 'kd_tree', 'brute'],\n",
    "            \"weights\" : ['uniform', 'distance'],\n",
    "            \"leaf_size\" : np.arange(1,60)\n",
    "        }\n",
    "        return Seismic_Bumps.randomCV(neigh, X, y, param_grid, 400, 6)\n",
    "    \n",
    "    def cv_DT(self, X, y):\n",
    "        dt = tree.DecisionTreeClassifier()\n",
    "        param_grid = {\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"splitter\" : ['best', 'random'],\n",
    "            \"min_samples_split\" : np.random.random_sample((100,)),\n",
    "            \"max_features\" : ['sqrt', 'log2', None],\n",
    "            \"class_weight\" : [None, 'balanced'],\n",
    "            \"min_samples_leaf\" : np.arange(1,6),\n",
    "            'max_depth': range(1, 20)\n",
    "        }\n",
    "        return Seismic_Bumps.randomCV(dt, X, y, param_grid, 400, 6)\n",
    "        \n",
    "    def cv_RandomForest(self, X, y):\n",
    "        rf = RandomForestClassifier()\n",
    "        param_grid = {\n",
    "            \"n_estimators\" : [10*x for x in np.arange(1,50)],\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"min_samples_split\" : np.random.random_sample((100,)),\n",
    "            \"max_features\" : ['sqrt', 'log2', None],\n",
    "            \"min_samples_leaf\" : np.arange(1,6),\n",
    "            'max_depth': range(1, 20)            \n",
    "        }\n",
    "        return Seismic_Bumps.randomCV(rf, X, y, param_grid, 40, 6)\n",
    "        \n",
    "        \n",
    "    def cv_adaBoost(self, X, y):\n",
    "        scorer = make_scorer(Seismic_Bumps.score_func)\n",
    "        ada_boost = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "        param_grid = {'n_estimators': range(1, 100)}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(ada_boost, param_grid, verbose=1, cv=3, scoring=scorer)\n",
    "        gridcv.fit(X, y)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% AUPR on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "        \n",
    "    def cv_logReg(self, X, y):\n",
    "        lr = LogisticRegression()\n",
    "        param_grid = {\n",
    "            \"penalty\" : ['l1', 'l2'],\n",
    "            \"C\" : np.random.rand(60),\n",
    "            \"solver\" : [ 'liblinear']\n",
    "        }\n",
    "        return Seismic_Bumps.randomCV(lr, X, y, param_grid, 400, 6)\n",
    "        \n",
    "    def cv_GNB(self, X, y):\n",
    "        gnb = GaussianNB()\n",
    "        param_grid = {\n",
    "            \"var_smoothing\" : np.random.random_sample((100,))\n",
    "        }\n",
    "        return Seismic_Bumps.randomCV(gnb, X, y, param_grid, 100, 6)\n",
    "    \n",
    "    def cv_NN(self, X, y):\n",
    "        nn = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(50,),\n",
    "                                           solver='sgd', batch_size=100, max_iter=10,\n",
    "                                           learning_rate_init=.01, momentum=0.9, alpha=0.05,\n",
    "                                           verbose=False, random_state=0)\n",
    "\n",
    "        param_grid ={\n",
    "                    'hidden_layer_sizes' : range(2,100),\n",
    "                    \"activation\" : ['identity', 'logistic', 'tanh', 'relu']\n",
    "                    }\n",
    "        return Seismic_Bumps.randomCV(nn, X, y, param_grid, 200, 6)\n",
    "        \n",
    "    def randomCV(clf, X, y, param_grid, n_iter, cv):\n",
    "        scorer = make_scorer(Seismic_Bumps.score_func)\n",
    "        random_search = RandomizedSearchCV(clf, param_distributions = param_grid, n_iter = n_iter, cv = cv, iid = False, \n",
    "                                           scoring = scorer)\n",
    "        random_search.fit(X, y)\n",
    "        print(random_search.best_params_)        \n",
    "        Seismic_Bumps.report(random_search.cv_results_)\n",
    "        return random_search.best_estimator_\n",
    "    \n",
    "    def report(results, n_top=1):\n",
    "        for i in range(1, n_top + 1):\n",
    "            candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "            k = 0\n",
    "            for candidate in candidates:                                \n",
    "                print(\"Model with rank: {0}\".format(i))\n",
    "                print(\"AUPR on validation data: {0:.3f} (std: {1:.3f})\".format(\n",
    "                      results['mean_test_score'][candidate],\n",
    "                      results['std_test_score'][candidate]))\n",
    "                print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "                print(\"\")\n",
    "                k += 1\n",
    "                if k == 3:\n",
    "                    break\n",
    "    \n",
    "    def predict(self, model, X_test, y_test):\n",
    "        predict = model.predict(X_test)\n",
    "        accuracy = sklearn.metrics.accuracy_score(y_test, predict)\n",
    "        print(\"Final Accuracy on test data : \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class German_CC:\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def read_data(self):\n",
    "        # reading data and doing 80-20 split\n",
    "        data = np.loadtxt('german.data-numeric')\n",
    "        X = data[:,:-1]\n",
    "        y = data[:,-1]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "        return X_train, y_train, X_test, y_test\n",
    "\n",
    "        \n",
    "    def cv_SVM(self, X, y):\n",
    "        scorer = make_scorer(precision_score)\n",
    "        C_grid = [0.1, 1, 10]\n",
    "        gamma_grid = np.logspace(-2, 1, 4)[0:3]\n",
    "        svm = sklearn.svm.SVC(kernel='rbf')\n",
    "        param_grid = { 'C' : C_grid, 'gamma' : gamma_grid, 'kernel' : ['rbf', 'sigmoid']}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(svm, param_grid, n_jobs=-1, verbose=1, cv=3, scoring = scorer)\n",
    "        gridcv.fit(X_train, y_train)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "\n",
    "    def cv_Knn(self, X, y):\n",
    "        neigh = KNeighborsClassifier()\n",
    "        param_grid = {\n",
    "            \"n_neighbors\" : np.arange(1,20),\n",
    "            \"algorithm\" : ['ball_tree', 'kd_tree', 'brute'],\n",
    "            \"weights\" : ['uniform', 'distance'],\n",
    "            \"leaf_size\" : np.arange(1,60)\n",
    "        }\n",
    "        return German_CC.randomCV(neigh, X, y, param_grid, 400, 6)\n",
    "    \n",
    "    def cv_DT(self, X, y):        \n",
    "        dt = tree.DecisionTreeClassifier()\n",
    "        param_grid = {\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"splitter\" : ['best', 'random'],\n",
    "            \"min_samples_split\" : np.random.random_sample((100,)),\n",
    "            \"max_features\" : ['auto', 'sqrt', 'log2', None],\n",
    "            \"class_weight\" : [None, 'balanced'],\n",
    "            \"presort\" : [True, False],\n",
    "            \"min_samples_leaf\" : np.arange(1,6),\n",
    "            'max_depth': range(1, 20)\n",
    "        }\n",
    "        return German_CC.randomCV(dt, X, y, param_grid, 400, 6)\n",
    "        \n",
    "    def cv_RandomForest(self, X, y):\n",
    "        rf = RandomForestClassifier()\n",
    "        param_grid = {\n",
    "            \"n_estimators\" : [10*x for x in np.arange(1,50)],\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"min_samples_split\" : np.random.random_sample((100,)),\n",
    "            \"max_features\" : ['auto', 'sqrt', 'log2', None],\n",
    "            #\"class_weight\" : [None, 'balanced'],\n",
    "            \"min_samples_leaf\" : np.arange(1,6),\n",
    "            #\"bootstrap\" : [True, False],\n",
    "            #\"oob_score\" : [True, False],\n",
    "            \"warm_start\" : [True, False],\n",
    "        }\n",
    "        return German_CC.randomCV(rf, X, y, param_grid, 40, 6)\n",
    "        \n",
    "    def cv_adaBoost(self, X, y):\n",
    "        scorer = make_scorer(precision_score)\n",
    "        ada_boost = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "        param_grid = {'n_estimators': range(1, 100)}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(ada_boost, param_grid, verbose=1, cv=3, scoring=scorer)\n",
    "        gridcv.fit(X, y)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "    \n",
    "    def cv_logReg(self, X, y):\n",
    "        lr = LogisticRegression()\n",
    "        param_grid = {\n",
    "            \"penalty\" : ['l1', 'l2'],\n",
    "            #\"dual\" : [True, False],\n",
    "            \"C\" : np.random.rand(60),\n",
    "            \"fit_intercept\" : [True, False],\n",
    "            \"warm_start\" : [True, False],\n",
    "            \"multi_class\" : ['ovr', 'auto'],\n",
    "            \"solver\" : [ 'liblinear']\n",
    "        }\n",
    "        return German_CC.randomCV(lr, X, y, param_grid, 400, 6)\n",
    "        \n",
    "    def cv_GNB(self, X, y):\n",
    "        gnb = GaussianNB()\n",
    "        param_grid = {\n",
    "            \"var_smoothing\" : np.random.random_sample((100,))\n",
    "        }\n",
    "        return German_CC.randomCV(gnb, X, y, param_grid, 100, 6)\n",
    "    \n",
    "    def cv_NN(self, X, y):\n",
    "        nn = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(50,),\n",
    "                                           solver='sgd', batch_size=100, max_iter=10,\n",
    "                                           learning_rate_init=.01, momentum=0.9, alpha=0.05,\n",
    "                                           verbose=False, random_state=0)\n",
    "\n",
    "        param_grid ={\n",
    "                    'hidden_layer_sizes' : range(2,100),\n",
    "                    \"activation\" : ['identity', 'logistic', 'tanh', 'relu']\n",
    "                    }\n",
    "        return German_CC.randomCV(nn, X, y, param_grid, 200, 6)\n",
    "        \n",
    "    def randomCV(clf, X, y, param_grid, n_iter, cv):\n",
    "        scorer = make_scorer(precision_score)\n",
    "        random_search = RandomizedSearchCV(clf, param_distributions = param_grid, n_iter = n_iter, cv = cv, iid = False, \n",
    "                                           scoring = scorer)\n",
    "        random_search.fit(X, y)        \n",
    "        German_CC.report(random_search.cv_results_)\n",
    "        return random_search.best_estimator_\n",
    "    \n",
    "    def report(results, n_top=1):\n",
    "        for i in range(1, n_top + 1):\n",
    "            candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "            k = 0\n",
    "            for candidate in candidates:                \n",
    "                print(\"Model with rank: {0}\".format(i))\n",
    "                print(\"Mean precision score on validation data: {0:.3f} (std: {1:.3f})\".format(\n",
    "                      results['mean_test_score'][candidate],\n",
    "                      results['std_test_score'][candidate]))\n",
    "                print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "                print(\"\")\n",
    "                k += 1\n",
    "                if k == 3:\n",
    "                    break\n",
    "                \n",
    "    def predict(self, model, X_test, y_test):\n",
    "        predict = model.predict(X_test)\n",
    "        accuracy = sklearn.metrics.accuracy_score(y_test, predict)\n",
    "        print(\"Final Accuracy on test data : \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Retinopathy:\n",
    "    \n",
    "    def __init__(self):\n",
    "        return\n",
    "    \n",
    "    def read_data(self):\n",
    "        data_set =  np.loadtxt('messidor_features.csv', delimiter=',', skiprows=0)\n",
    "        X = data_set[:,0:-1]\n",
    "        y = data_set[:,-1]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)\n",
    "        return X_train, y_train, X_test, y_test\n",
    "        \n",
    "    def preprocessing(self, X_train, X_test):\n",
    "        # preprocessing using standard scaler\n",
    "        scaler = sklearn.preprocessing.StandardScaler()\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.transform(X_test)\n",
    "        return X_train, X_test\n",
    "        \n",
    "    def cv_SVM(self, X_train, y_train):\n",
    "        C_grid = [0.1, 1, 10]\n",
    "        gamma_grid = np.logspace(-2, 1, 4)[0:3]\n",
    "        svm = sklearn.svm.SVC(kernel='rbf')\n",
    "        param_grid = { 'C' : C_grid, 'gamma' : gamma_grid}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(svm, param_grid, verbose=1, cv=3)\n",
    "        gridcv.fit(X_train, y_train)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "\n",
    "    def cv_Knn(self, X_train, y_train):\n",
    "        neigh = KNeighborsClassifier()\n",
    "        param_grid = {'n_neighbors': range(1, 20)}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(neigh, param_grid, verbose=1, cv=3)\n",
    "        gridcv.fit(X_train, y_train)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "    \n",
    "    def cv_DT(self, X_train, y_train):\n",
    "        decision_tree = tree.DecisionTreeClassifier(random_state=0)\n",
    "        param_grid = {'max_depth': range(1, 20)}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(decision_tree, param_grid, verbose=1, cv=3)\n",
    "        gridcv.fit(X_train, y_train)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "        \n",
    "    def cv_RandomForest(self, X_train, y_train):\n",
    "        random_decision = RandomForestClassifier(random_state=0)\n",
    "        param_grid = {'n_jobs': range(1, 10)}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(random_decision, param_grid, verbose=1, cv=3)\n",
    "        gridcv.fit(X_train, y_train)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "        \n",
    "    def cv_adaBoost(self, X_train, y_train):\n",
    "        ada_boost = AdaBoostClassifier(n_estimators=50, learning_rate=1)\n",
    "        param_grid = {'n_estimators': range(1, 100)}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(ada_boost, param_grid, verbose=1, cv=3)\n",
    "        gridcv.fit(X_train, y_train)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "    \n",
    "    def cv_logReg(self, X_train, y_train):\n",
    "        logreg = LogisticRegression(C=1e5, solver='lbfgs', multi_class='multinomial',  max_iter=400)\n",
    "        param_grid = {'C': np.logspace(-4, 3, 20)}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(logreg, param_grid, verbose=1, cv=3)\n",
    "        gridcv.fit(X_train, y_train)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "        \n",
    "    def cv_GNB(self, X_train, y_train):\n",
    "        gnb = GaussianNB()\n",
    "        param_grid ={}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(gnb, param_grid, verbose=1, cv=3)\n",
    "        gridcv.fit(X_train, y_train)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "    \n",
    "    def cv_NN(self, X_train, y_train):\n",
    "        nn = sklearn.neural_network.MLPClassifier(hidden_layer_sizes=(50,),\n",
    "                                           solver='sgd', batch_size=100, max_iter=10,\n",
    "                                           learning_rate_init=.01, momentum=0.9, alpha=0.05,\n",
    "                                           verbose=True, random_state=0)\n",
    "\n",
    "        param_grid ={'hidden_layer_sizes' : (10,30,50,70,100)}\n",
    "        gridcv = sklearn.model_selection.GridSearchCV(nn, param_grid, verbose=1, cv=3)\n",
    "        gridcv.fit(X_train, y_train)\n",
    "        print(\"best parameters:\", gridcv.best_params_)\n",
    "        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "        return gridcv.best_estimator_\n",
    "    \n",
    "    def predict(self, model, X_test, y_test):\n",
    "        predict = model.predict(X_test)\n",
    "        accuracy = sklearn.metrics.accuracy_score(y_test, predict)\n",
    "        print(\"Final Accuracy on test data : \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.io import arff\n",
    "from io import StringIO\n",
    "import sklearn\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning) \n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "from sklearn.metrics import make_scorer\n",
    "import time\n",
    "\n",
    "\n",
    "class statlog_aus:\n",
    "    def read(self, a):\n",
    "        f = open(a,\"r\")\n",
    "        c = StringIO(f.read())\n",
    "        return np.loadtxt(c)\n",
    "    def report(self, results, n_top=3):\n",
    "        for i in range(1, n_top + 1):\n",
    "            candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "            j = 0\n",
    "            for candidate in candidates:\n",
    "                print(\"Model with rank: {0}\".format(i))\n",
    "                print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                      results['mean_test_score'][candidate],\n",
    "                      results['std_test_score'][candidate]))\n",
    "                print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "                print(\"\")\n",
    "                if j > 1:\n",
    "                    break\n",
    "                j+=1\n",
    "\n",
    "\n",
    "    def randomCV(self, clf, X, y, param_grid, n_iter, cv):\n",
    "        random_search = RandomizedSearchCV(clf, param_distributions = param_grid,\n",
    "                        n_iter = n_iter, cv = cv, iid = False, n_jobs = -1)\n",
    "        random_search.fit(X, y)\n",
    "        self.report(random_search.cv_results_)\n",
    "        return random_search.best_params_\t\t\n",
    "\n",
    "    def KNN(self, X, y):\n",
    "\n",
    "        neigh = KNeighborsClassifier()\n",
    "        param_grid = {\n",
    "            \"n_neighbors\" : np.arange(1,20),\n",
    "            \"algorithm\" : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            \"weights\" : ['uniform', 'distance'],\n",
    "            \"leaf_size\" : np.arange(1,60)\n",
    "        }\n",
    "        return self.randomCV(neigh, X, y, param_grid, 400, 6)\n",
    "\n",
    "    def SVM(self, X, y):\n",
    "\n",
    "    #        C_grid = [0.1, 1, 10]\n",
    "    #        gamma_grid = np.logspace(-2, 1, 4)[0:3]\n",
    "    #        svm_C = svm.SVC(kernel='poly')\n",
    "    #        param_grid = { 'C' : C_grid, 'gamma' : gamma_grid, \"kernel\" : ['poly', 'rbf', 'sigmoid'], }\n",
    "    #        gridcv = GridSearchCV(svm_C, param_grid, verbose=1, cv=3)\n",
    "    #        gridcv.fit(X, y)\n",
    "    #        print(\"best parameters:\", gridcv.best_params_)\n",
    "    #        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "\n",
    "        svm_C = svm.SVC()\n",
    "        param_grid = {\n",
    "            \"kernel\" : ['linear', 'rbf', 'sigmoid'],\n",
    "            \"gamma\" : ['scale', 'auto'],\n",
    "            \"degree\" : np.arange(10),\n",
    "            \"coef0\" : np.random.rand(60)*10,\n",
    "            \"shrinking\" : [False, True],\n",
    "            \"decision_function_shape\" : ['ovo','ovr']\n",
    "        }\n",
    "        return self.randomCV(svm_C, X, y, param_grid, 4, 6)\n",
    "\n",
    "    def DT(self, X, y):\n",
    "        dt = DecisionTreeClassifier()\n",
    "        param_grid = {\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"splitter\" : ['best', 'random'],\n",
    "            \"min_samples_split\" : np.random.random_sample((100,)),\n",
    "            \"max_features\" : ['auto', 'sqrt', 'log2', None],\n",
    "            \"class_weight\" : [None, 'balanced'],\n",
    "            \"presort\" : [True, False],\n",
    "            \"min_samples_leaf\" : np.arange(1,6)\n",
    "        }\n",
    "        return self.randomCV(dt, X, y, param_grid, 400, 6)\n",
    "\n",
    "    def RF(self, X, y):\n",
    "        rf = RandomForestClassifier()\n",
    "        param_grid = {\n",
    "            \"n_estimators\" : [10*x for x in np.arange(1,50)],\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"min_samples_split\" : np.random.random_sample((100,)),\n",
    "            \"max_features\" : ['auto', 'sqrt', 'log2', None],\n",
    "    #\t\t\"class_weight\" : [None, 'balanced'],\n",
    "            \"min_samples_leaf\" : np.arange(1,6),\n",
    "    #\t\t\"bootstrap\" : [True, False],\n",
    "    #\t\t\"oob_score\" : [True, False],\n",
    "            \"warm_start\" : [True, False],\n",
    "        }\n",
    "        return self.randomCV(rf, X, y, param_grid, 40, 6)\n",
    "\n",
    "    def Ada(self, X, y):\n",
    "        ada = AdaBoostClassifier(algorithm = \"SAMME\")\n",
    "        param_grid = {\n",
    "    #\t\t\"base_estimator\" : ['classes', 'n_classes_', None],\n",
    "            \"n_estimators\" : [10*x for x in np.arange(1,50)]\n",
    "    #\t\t\"learning_rate\" : [10*x for x in np.random.random_sample((100,))]\n",
    "    #\t\t\"algorithm\" : ['SAMME']\n",
    "        }\n",
    "        return self.randomCV(ada, X, y, param_grid, 40, 6)\n",
    "\n",
    "    def LR(self, X, y):\n",
    "        lr = LogisticRegression()\n",
    "        param_grid = {\n",
    "            \"penalty\" : ['l1', 'l2'],\n",
    "    #\t\t\"dual\" : [True, False],\n",
    "            \"C\" : np.random.rand(60),\n",
    "            \"fit_intercept\" : [True, False],\n",
    "            \"warm_start\" : [True, False],\n",
    "            \"multi_class\" : ['ovr', 'auto'],\n",
    "            \"solver\" : [ 'liblinear']\n",
    "        }\n",
    "        return self.randomCV(lr, X, y, param_grid, 400, 6)\n",
    "\n",
    "    def GNB(self, X, y):\n",
    "        gnb = GaussianNB()\n",
    "        param_grid = {\n",
    "            \"var_smoothing\" : np.random.random_sample((100,))\n",
    "        }\n",
    "        return self.randomCV(gnb, X, y, param_grid, 100, 6)\n",
    "\n",
    "    def NN(self, X, y):\n",
    "        nn = MLPClassifier()\n",
    "        param_grid = {\n",
    "            \"hidden_layer_sizes\" : np.arange(2,200),\n",
    "            \"activation\" : ['identity', 'logistic', 'tanh', 'relu'],\n",
    "            \"solver\" : ['lbfgs', 'sgd', 'adam'],\n",
    "    #\t\t\"verbose\" : [True, False],\n",
    "            \"warm_start\" : [False, True]\n",
    "        }\n",
    "        return self.randomCV(nn, X, y, param_grid, 200, 6)\n",
    "    def start(self):\n",
    "        data = self.read('australian.dat')\n",
    "        y = data[:,-1]\n",
    "        x = data[:,0:data.shape[1] -1]\n",
    "\n",
    "        np.random.shuffle(data)\n",
    "\n",
    "        x_test, x_train = np.split(x, [105])\n",
    "        y_test, y_train = np.split(y, [105])\n",
    "\n",
    "        scaler = StandardScaler()                         # scaling features\n",
    "        scaler.fit(x_train)\n",
    "        x_train = scaler.transform(x_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "        \n",
    "        # --------------------> KNN\n",
    "\n",
    "        param = self.KNN(x_train,y_train)\n",
    "        knn_c = KNeighborsClassifier().set_params(**param)\n",
    "        knn_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",knn_c.score(x_test, y_test))\n",
    "        \n",
    "        # --------------------> Decision Tree\n",
    "\n",
    "        param = self.DT(x_train,y_train)\n",
    "        dt_c = DecisionTreeClassifier().set_params(**param)\n",
    "        dt_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",dt_c.score(x_test, y_test))\n",
    "        \n",
    "        # --------------------> Random Forest\n",
    "\n",
    "        param = self.RF(x_train,y_train)\n",
    "        rf_c = RandomForestClassifier().set_params(**param)\n",
    "        rf_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",rf_c.score(x_test, y_test))\n",
    "        \n",
    "        # --------------------> Adaboost\n",
    "\n",
    "        param = self.Ada(x_train,y_train)\n",
    "        ada_c = AdaBoostClassifier().set_params(**param)\n",
    "        ada_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",ada_c.score(x_test, y_test))\n",
    "        \n",
    "        # ---------------------> Logistic regression\n",
    "\n",
    "        param = self.LR(x_train,y_train)\n",
    "        lr_c = LogisticRegression().set_params(**param)\n",
    "        lr_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",lr_c.score(x_test, y_test))\n",
    "        \n",
    "        # ---------------------> Gaussian NB\n",
    "\n",
    "        param = self.GNB(x_train,y_train)\n",
    "        gnb_c = GaussianNB().set_params(**param)\n",
    "        gnb_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",gnb_c.score(x_test, y_test))\n",
    "        \n",
    "        # ---------------------> Neural Network\n",
    "\n",
    "        param = self.NN(x_train,y_train)\n",
    "        nn_c = MLPClassifier().set_params(**param)\n",
    "        nn_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",nn_c.score(x_test, y_test))\n",
    "        \n",
    "        ## --------------------> SVM\n",
    "\n",
    "        param = self.SVM(x_train,y_train)\n",
    "        svm_c = svm.SVC().set_params(**param)\n",
    "        svm_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",svm_c.score(x_test, y_test))\n",
    "        \n",
    "        \n",
    "class steel:\n",
    "    def read(self,a):\n",
    "      f = open(a,\"r\")\n",
    "      c = StringIO(f.read())\n",
    "      return np.loadtxt(c)\n",
    "\n",
    "    def report(self, results, n_top=3):\n",
    "        for i in range(1, n_top + 1):\n",
    "            candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "            j = 0\n",
    "            for candidate in candidates:\n",
    "                print(\"Model with rank: {0}\".format(i))\n",
    "                print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                      results['mean_test_score'][candidate],\n",
    "                      results['std_test_score'][candidate]))\n",
    "                print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "                print(\"\")\n",
    "                if j > 1:\n",
    "                    break\n",
    "                j+=1\n",
    "\n",
    "\n",
    "    def randomCV(self, clf, X, y, param_grid, n_iter, cv):\n",
    "        random_search = RandomizedSearchCV(clf, param_distributions = param_grid,\n",
    "                        n_iter = n_iter, cv = cv, iid = False, n_jobs = -1)\n",
    "        random_search.fit(X, y)\n",
    "        self.report(random_search.cv_results_)\n",
    "        return random_search.best_params_\t\t\n",
    "\n",
    "    def KNN(self, X, y):\n",
    "\n",
    "        neigh = KNeighborsClassifier()\n",
    "        param_grid = {\n",
    "            \"n_neighbors\" : np.arange(1,20),\n",
    "            \"algorithm\" : ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "            \"weights\" : ['uniform', 'distance'],\n",
    "            \"leaf_size\" : np.arange(1,60)\n",
    "        }\n",
    "        return self.randomCV(neigh, X, y, param_grid, 400, 6)\n",
    "\n",
    "    def SVM(self, X, y):\n",
    "\n",
    "    #        C_grid = [0.1, 1, 10]\n",
    "    #        gamma_grid = np.logspace(-2, 1, 4)[0:3]\n",
    "    #        svm_C = svm.SVC(kernel='poly')\n",
    "    #        param_grid = { 'C' : C_grid, 'gamma' : gamma_grid, \"kernel\" : ['poly', 'rbf', 'sigmoid'], }\n",
    "    #        gridcv = GridSearchCV(svm_C, param_grid, verbose=1, cv=3)\n",
    "    #        gridcv.fit(X, y)\n",
    "    #        print(\"best parameters:\", gridcv.best_params_)\n",
    "    #        print(\"%.1f%% accuracy on validation sets (average)\" % (gridcv.best_score_*100))\n",
    "\n",
    "        svm_C = svm.SVC()\n",
    "        param_grid = {\n",
    "            \"kernel\" : ['linear', 'rbf', 'sigmoid'],\n",
    "            \"gamma\" : ['scale', 'auto'],\n",
    "            \"degree\" : np.arange(10),\n",
    "            \"coef0\" : np.random.rand(60)*10,\n",
    "            \"shrinking\" : [False, True],\n",
    "            \"decision_function_shape\" : ['ovo','ovr']\n",
    "        }\n",
    "        return self.randomCV(svm_C, X, y, param_grid, 50, 6)\n",
    "\n",
    "    def DT(self, X, y):\n",
    "        dt = DecisionTreeClassifier()\n",
    "        param_grid = {\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"splitter\" : ['best', 'random'],\n",
    "            \"min_samples_split\" : np.random.random_sample((100,)),\n",
    "            \"max_features\" : ['auto', 'sqrt', 'log2', None],\n",
    "            \"class_weight\" : [None, 'balanced'],\n",
    "            \"presort\" : [True, False],\n",
    "            \"min_samples_leaf\" : np.arange(1,6)\n",
    "        }\n",
    "        return self.randomCV(dt, X, y, param_grid, 400, 6)\n",
    "\n",
    "    def RF(self, X, y):\n",
    "        rf = RandomForestClassifier()\n",
    "        param_grid = {\n",
    "            \"n_estimators\" : [10*x for x in np.arange(1,50)],\n",
    "            \"criterion\" : ['gini', 'entropy'],\n",
    "            \"min_samples_split\" : np.random.random_sample((100,)),\n",
    "            \"max_features\" : ['auto', 'sqrt', 'log2', None],\n",
    "    #\t\t\"class_weight\" : [None, 'balanced'],\n",
    "            \"min_samples_leaf\" : np.arange(1,6),\n",
    "    #\t\t\"bootstrap\" : [True, False],\n",
    "    #\t\t\"oob_score\" : [True, False],\n",
    "            \"warm_start\" : [True, False],\n",
    "        }\n",
    "        return self.randomCV(rf, X, y, param_grid, 40, 6)\n",
    "\n",
    "    def Ada(self, X, y):\n",
    "        ada = AdaBoostClassifier(algorithm = \"SAMME\")\n",
    "        param_grid = {\n",
    "    #\t\t\"base_estimator\" : ['classes', 'n_classes_', None],\n",
    "            \"n_estimators\" : [10*x for x in np.arange(1,50)]\n",
    "    #\t\t\"learning_rate\" : [10*x for x in np.random.random_sample((100,))]\n",
    "    #\t\t\"algorithm\" : ['SAMME']\n",
    "        }\n",
    "        return self.randomCV(ada, X, y, param_grid, 40, 6)\n",
    "\n",
    "    def LR(self, X, y):\n",
    "        lr = LogisticRegression()\n",
    "        param_grid = {\n",
    "            \"penalty\" : ['l1', 'l2'],\n",
    "    #\t\t\"dual\" : [True, False],\n",
    "            \"C\" : np.random.rand(60),\n",
    "            \"fit_intercept\" : [True, False],\n",
    "            \"warm_start\" : [True, False],\n",
    "            \"multi_class\" : ['ovr', 'auto'],\n",
    "            \"solver\" : [ 'liblinear']\n",
    "        }\n",
    "        return self.randomCV(lr, X, y, param_grid, 400, 6)\n",
    "\n",
    "    def GNB(self, X, y):\n",
    "        gnb = GaussianNB()\n",
    "        param_grid = {\n",
    "            \"var_smoothing\" : np.random.random_sample((100,))\n",
    "        }\n",
    "        return self.randomCV(gnb, X, y, param_grid, 100, 6)\n",
    "\n",
    "    def NN(self, X, y):\n",
    "        nn = MLPClassifier()\n",
    "        param_grid = {\n",
    "            \"hidden_layer_sizes\" : np.arange(2,200),\n",
    "            \"activation\" : ['identity', 'logistic', 'tanh', 'relu'],\n",
    "            \"solver\" : ['lbfgs', 'sgd', 'adam'],\n",
    "    #\t\t\"verbose\" : [True, False],\n",
    "            \"warm_start\" : [False, True]\n",
    "        }\n",
    "        return self.randomCV(nn, X, y, param_grid, 200, 6)\n",
    "\n",
    "    def start(self):\n",
    "        data = self.read('Faults.NNA')\n",
    "        print(data.shape)\n",
    "\n",
    "        np.random.shuffle(data)\n",
    "\n",
    "        y = data[:,-1]\n",
    "        x = data[:,0:data.shape[1] -1]\n",
    "\n",
    "        x_test, x_train = np.split(x, [290])\n",
    "        y_test, y_train = np.split(y, [290])\n",
    "\n",
    "        scaler = StandardScaler()                         # scaling features\n",
    "        scaler.fit(x_train)\n",
    "        x_train = scaler.transform(x_train)\n",
    "        x_test = scaler.transform(x_test)\n",
    "        \n",
    "        # --------------------> KNN\n",
    "\n",
    "        param = self.KNN(x_train,y_train)\n",
    "        knn_c = KNeighborsClassifier().set_params(**param)\n",
    "        knn_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",knn_c.score(x_test, y_test))        \n",
    "        \n",
    "        # --------------------> Decision Tree\n",
    "\n",
    "        param = self.DT(x_train,y_train)\n",
    "        dt_c = DecisionTreeClassifier().set_params(**param)\n",
    "        dt_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",dt_c.score(x_test, y_test))        \n",
    "        \n",
    "        # --------------------> Random Forest\n",
    "\n",
    "        param = self.RF(x_train,y_train)\n",
    "        rf_c = RandomForestClassifier().set_params(**param)\n",
    "        rf_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",rf_c.score(x_test, y_test))\n",
    "        \n",
    "        # --------------------> Adaboost\n",
    "\n",
    "        param = self.Ada(x_train,y_train)\n",
    "        ada_c = AdaBoostClassifier().set_params(**param)\n",
    "        ada_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",ada_c.score(x_test, y_test))\n",
    "        \n",
    "        # ---------------------> Logistic regression\n",
    "\n",
    "        param = self.LR(x_train,y_train)\n",
    "        lr_c = LogisticRegression().set_params(**param)\n",
    "        lr_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",lr_c.score(x_test, y_test))\n",
    "        \n",
    "        # ---------------------> Gaussian NB\n",
    "\n",
    "        param = self.GNB(x_train,y_train)\n",
    "        gnb_c = GaussianNB().set_params(**param)\n",
    "        gnb_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",gnb_c.score(x_test, y_test))\n",
    "        \n",
    "        # ---------------------> Neural Network\n",
    "\n",
    "#        This does converge and gives good result but takes a long time\n",
    "        \n",
    "#         param = self.NN(x_train,y_train)\n",
    "#         nn_c = MLPClassifier().set_params(**param)\n",
    "#         nn_c.fit(x_train, y_train)\n",
    "\n",
    "#         print(\"Score with test data\",nn_c.score(x_test, y_test))\n",
    "        \n",
    "\n",
    "        ## --------------------> SVM\n",
    "\n",
    "        param = self.SVM(x_train,y_train)\n",
    "        svm_c = svm.SVC().set_params(**param)\n",
    "        svm_c.fit(x_train, y_train)\n",
    "\n",
    "        print(\"Score with test data\",svm_c.score(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
